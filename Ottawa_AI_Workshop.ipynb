{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ottawa AI Workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcshapiro/wordEmbeddingOttawaAiAlliance/blob/master/Ottawa_AI_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFkbk9KfK02A",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "In this notebook, you will play around with various natural language processing technologies and tools, focusing on applications.\n",
        "\n",
        "The exercises in this notebook are:\n",
        "- Have a look at the embedding projector\n",
        "- Training a FastText model to label sentences\n",
        "- Training a character-level Keras model to predict the type of a business from the name of the business\n",
        "- Using a GloVe model from spaCy to analyze synthetic medical chart notes (text) and then correct any associated medical billing errors\n",
        "- Customizing FastText to a specialized corpus (first using the built-in approach, and then using sidecar)\n",
        "\n",
        "Let's have a look at the tensorflow embedding projector at this link:\n",
        "http://projector.tensorflow.org/\n",
        "\n",
        "Note: This notebook has a massive selection bias. Most things you try won't work. This notebook is set up to show you highly tuned examples that do work, but alas life is bitterness, and so don't be surprised when you jump in and everything seems a lot harder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgBj1Y1GeLnq",
        "colab_type": "text"
      },
      "source": [
        "# FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuKPbUy_SwwJ",
        "colab_type": "text"
      },
      "source": [
        "Let's install FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiPN-sIYShi1",
        "colab_type": "code",
        "outputId": "551f0bea-c516-4132-8028-3f369c918671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "!pip3 install fasttext"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (41.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2387629 sha256=5ae97e8529ed052bb7336cb4293da9f4757a449ad711e2b7c06f8ce1a1632ee4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyzZwUFsS3zG",
        "colab_type": "text"
      },
      "source": [
        "Let's mount Google Drive as our file system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEklpCFNSnE-",
        "colab_type": "code",
        "outputId": "3fd6ed8e-4675-4958-e102-c13f4a85ff6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLmB7FxBTgPX",
        "colab_type": "text"
      },
      "source": [
        "Now let's get some data to work on (recipes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6_ak1mAWPXS",
        "colab_type": "code",
        "outputId": "0d3ee59a-e6e6-483d-f7c1-e55d0d3c17e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "base_dir=\"/content/gdrive/My\\ Drive/AuditMap_workshop/\"\n",
        "py_base_dir=base_dir.replace(\"\\\\\",\"\")\n",
        "print(py_base_dir)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/AuditMap_workshop/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdXiZxwsTk6a",
        "colab_type": "code",
        "outputId": "fe4f1732-03c6-45b3-9ae9-36086d7df6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "!mkdir {base_dir}\n",
        "!cd {base_dir} && wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz \n",
        "!cd {base_dir} && tar xvzf cooking.stackexchange.tar.gz\n",
        "!ls -l {base_dir}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/My Drive/AuditMap_workshop/’: File exists\n",
            "--2019-11-27 19:32:05--  https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:6a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 457609 (447K) [application/x-tar]\n",
            "Saving to: ‘cooking.stackexchange.tar.gz.6’\n",
            "\n",
            "cooking.stackexchan 100%[===================>] 446.88K  1009KB/s    in 0.4s    \n",
            "\n",
            "2019-11-27 19:32:06 (1009 KB/s) - ‘cooking.stackexchange.tar.gz.6’ saved [457609/457609]\n",
            "\n",
            "cooking.stackexchange.id\n",
            "cooking.stackexchange.txt\n",
            "readme.txt\n",
            "total 16729\n",
            "-rw------- 1 root root   90095 Apr 28  2017 cooking.stackexchange.id\n",
            "-rw------- 1 root root  457609 Jan 18  2019 cooking.stackexchange.tar.gz\n",
            "-rw------- 1 root root  457609 Jan 18  2019 cooking.stackexchange.tar.gz.1\n",
            "-rw------- 1 root root  457609 Jan 18  2019 cooking.stackexchange.tar.gz.2\n",
            "-rw------- 1 root root  457609 Jan 18  2019 cooking.stackexchange.tar.gz.3\n",
            "-rw------- 1 root root  457609 Jan 18  2019 cooking.stackexchange.tar.gz.4\n",
            "-rw------- 1 root root  457609 Jan 18  2019 cooking.stackexchange.tar.gz.5\n",
            "-rw------- 1 root root  457609 Jan 18  2019 cooking.stackexchange.tar.gz.6\n",
            "-rw------- 1 root root 1401900 Apr 28  2017 cooking.stackexchange.txt\n",
            "-rw------- 1 root root 1129498 Nov 27 18:40 cooking.train\n",
            "-rw------- 1 root root  272402 Nov 27 18:40 cooking.valid\n",
            "-rw------- 1 root root 6382062 Nov 27 18:41 model_supervised.bin\n",
            "-rw------- 1 root root  661651 Nov 27 15:35 patientRecords1574868874.8411162.csv\n",
            "-rw------- 1 root root 1660424 Nov 27 15:35 patientRecords_1574868874.8411162.h5\n",
            "-rw------- 1 root root  660598 Nov 27 18:41 patientRecords1574880078.9748778.csv\n",
            "-rw------- 1 root root 1659176 Nov 27 18:41 patientRecords_1574880078.9748778.h5\n",
            "-rw------- 1 root root     743 May  1  2017 readme.txt\n",
            "drwx------ 3 root root    4096 Nov 27 19:23 sidecar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFvzcX6nVH0t",
        "colab_type": "text"
      },
      "source": [
        "Now let's split up the data into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht8FCFhSVGaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd {base_dir} && head -n 12404 cooking.stackexchange.txt > cooking.train\n",
        "!cd {base_dir} && tail -n 3000 cooking.stackexchange.txt > cooking.valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBvdO9J1VsmD",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the data with our eyes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWsWroixVrzs",
        "colab_type": "code",
        "outputId": "40c3ceba-37bb-44cb-87ab-63a2707f90c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!head -n 10 {base_dir}/cooking.stackexchange.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__label__sauce __label__cheese How much does potato starch affect a cheese sauce recipe?\n",
            "__label__food-safety __label__acidity Dangerous pathogens capable of growing in acidic environments\n",
            "__label__cast-iron __label__stove How do I cover up the white spots on my cast iron stove?\n",
            "__label__restaurant Michelin Three Star Restaurant; but if the chef is not there\n",
            "__label__knife-skills __label__dicing Without knife skills, how can I quickly and accurately dice vegetables?\n",
            "__label__storage-method __label__equipment __label__bread What's the purpose of a bread box?\n",
            "__label__baking __label__food-safety __label__substitutions __label__peanuts how to seperate peanut oil from roasted peanuts at home?\n",
            "__label__chocolate American equivalent for British chocolate terms\n",
            "__label__baking __label__oven __label__convection Fan bake vs bake\n",
            "__label__sauce __label__storage-lifetime __label__acidity __label__mayonnaise Regulation and balancing of readymade packed mayonnaise and other sauces\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unHC7vSYTlL5",
        "colab_type": "text"
      },
      "source": [
        "Now let's try out FastText for *supervised learning*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmAV0cp5TPBj",
        "colab_type": "code",
        "outputId": "54ae3534-f8f6-4ee1-83fb-8bb5cfc1c60c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import fasttext\n",
        "model = fasttext.train_supervised(input=py_base_dir+\"cooking.train\")\n",
        "model.save_model(py_base_dir+\"model_supervised.bin\")\n",
        "model.predict(\"What's the purpose of a bread box?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('__label__baking',), array([0.1147495]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ_VlGJieeIo",
        "colab_type": "text"
      },
      "source": [
        "# Keras + Character level embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFKJRZJcas5v",
        "colab_type": "text"
      },
      "source": [
        "OK, so now let's do something cooler than recipe label prediction... Let's predict company type from name of company...\n",
        "More here: https://towardsdatascience.com/deep-learning-magic-small-business-type-8ac484d8c3bf\n",
        "\n",
        "It uses character by character embedding as we see here: https://github.com/lemay-ai/smallCompanyType2.0/blob/master/smallCompanyType/smallCompanyType.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7CvDmB5W1yS",
        "colab_type": "code",
        "outputId": "f1cbfba6-f843-4a42-cde2-9f2c8e500f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "!pip3 install smallCompanyType"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: smallCompanyType in /usr/local/lib/python3.6/dist-packages (2.2)\n",
            "Requirement already satisfied: numpy>==1.13.3 in /usr/local/lib/python3.6/dist-packages (from smallCompanyType) (1.17.4)\n",
            "Requirement already satisfied: h5py>=2.7.1 in /usr/local/lib/python3.6/dist-packages (from smallCompanyType) (2.8.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from smallCompanyType) (0.0)\n",
            "Requirement already satisfied: Keras>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from smallCompanyType) (2.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.7.1->smallCompanyType) (1.12.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->smallCompanyType) (0.21.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.2->smallCompanyType) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.2->smallCompanyType) (1.3.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.2->smallCompanyType) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.2->smallCompanyType) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->smallCompanyType) (0.14.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfE05IWCbHTK",
        "colab_type": "code",
        "outputId": "45129cd5-6c17-4d10-fbd3-e220b61a0360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "import smallCompanyType as s\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', '.*tensorflow.*',)\n",
        "warnings.filterwarnings('ignore', '.*OneHotEncoder.*',)\n",
        "\n",
        "b=s.SmallCompanyType()\n",
        "texts=[\"Lemay.ai Night Club\",\"Farah's variety\",\"felding and associates\",\"Lemay.ai Consulting\", \"Jims Garage\"]\n",
        "for text in texts:\n",
        "    ctype = b.getCompanyType(text)\n",
        "    csubtype = b.getCompanySubtype(text)\n",
        "    print(text,\"is a\",ctype,csubtype)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Lemay.ai Night Club is a B2BC Entertainment Services\n",
            "Farah's variety is a B2C Retail Dealer\n",
            "felding and associates is a B2C Retail Dealer\n",
            "Lemay.ai Consulting is a B2BC Office\n",
            "Jims Garage is a B2C Retail Dealer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUum-yfBc0ao",
        "colab_type": "text"
      },
      "source": [
        "Well, that was not super satisfying, and sort of high-level. Can we go a bit deeper and get our hands dirtier? Why, yes! Yes we can!\n",
        "\n",
        "# spaCy's GloVe vectors\n",
        "\n",
        "Let's look at a model for detecting errors in medical billing codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUYMQer6czkp",
        "colab_type": "code",
        "outputId": "1ae0bc9f-a718-45e3-c357-ffa05065de16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "!pip install names\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting names\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/4e/f9cb7ef2df0250f4ba3334fbdabaa94f9c88097089763d8e85ada8092f84/names-0.3.0.tar.gz (789kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 2.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: names\n",
            "  Building wheel for names (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for names: filename=names-0.3.0-cp36-none-any.whl size=803688 sha256=bbfc34752a984c5f51a2a8c11e96b0cd1fa18c2b04952dcfcaaa9116c6991478\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/a5/e1/be3e0aaa6fa285575078fa2aafd9959b45bdbc8de8a6803aeb\n",
            "Successfully built names\n",
            "Installing collected packages: names\n",
            "Successfully installed names-0.3.0\n",
            "Collecting en_core_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 62.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp36-none-any.whl size=828255076 sha256=adf456519a55b21e9073c778912c4e0c647dc5bc174350aa4c2f7103a7230a2c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-31na8542/wheels/b4/d7/70/426d313a459f82ed5e06cc36a50e2bb2f0ec5cb31d8e0bdf09\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QORkmPZqeyfM",
        "colab_type": "text"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIiEJqNZdVV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "from random import shuffle  \n",
        "import pandas as pd\n",
        "import random\n",
        "import spacy\n",
        "import names\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-h9_pP0e3iq",
        "colab_type": "text"
      },
      "source": [
        "Define a helper function to statistically generate a dataset of patient records including notes and billing codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YETCEABqbeF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nextVisitAction(item,bodyParts,psychDisorders,vaccines):    \n",
        "    chance = (random.randint(1,100))\n",
        "    if item is 'bodyPart':\n",
        "        #15% chance that dr writes the patient was treated on body part (CODE1)\n",
        "        if chance <=15:\n",
        "            note='Patient '+random.choice(bodyParts)+' was injured and was treated.'\n",
        "            return note, 1\n",
        "        else:\n",
        "            return '', 0\n",
        "    elif item is 'psychDisorder':\n",
        "        #20% chance that dr writes the patient was treated for a mental disorder (CODE2)\n",
        "        if chance <=20:\n",
        "            note='Patient was diagnosed with and treated for '+random.choice(psychDisorders) +'.'\n",
        "            return note, 2\n",
        "        else:\n",
        "            return '', 0\n",
        "    elif item is 'vaccine':\n",
        "        if chance <=5:\n",
        "            #5% chance that dr writes the patient was treated with a vaccine (CODE3)\n",
        "            note='Patient was administered the vaccice '+random.choice(vaccines) +'.'\n",
        "            return note, 3\n",
        "        else:\n",
        "            return '', 0\n",
        "    elif item is 'hasCold':\n",
        "        if chance <=15:\n",
        "            return 'It appears the patient has a mild virus.', 0\n",
        "        else:\n",
        "            return '', 0\n",
        "    elif item is 'catAllergy':\n",
        "        if chance <=15:\n",
        "            if chance <=7:\n",
        "                return 'The patient is mildly allergic to cats.', 0\n",
        "            return 'The patient is dealthly allergic to cats.', 0\n",
        "        else:\n",
        "            return '', 0\n",
        "    elif item is 'dogAllergy':\n",
        "        if chance <=15:\n",
        "            if chance <=7:\n",
        "                return 'The patient is mildly allergic to dogs.', 0\n",
        "            return 'The patient is dealthly allergic to dogs.', 0\n",
        "        else:\n",
        "            return '', 0\n",
        "    elif item is 'lactoseIntolerant':\n",
        "        if chance <=15:\n",
        "            if chance <=7:\n",
        "                return 'The patient is lactose intolerant.', 0\n",
        "            return 'The patient is very lactose intolerant.', 0\n",
        "        else:\n",
        "            return '', 0\n",
        "    elif item is 'looksPale':\n",
        "        if chance <=15:\n",
        "            return 'The patient looks very pale.', 0\n",
        "        else:\n",
        "            return '', 0\n",
        "    return item+'MISTAKE. Probably a BUG. WHAAAA!', 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwVRDY9_fONg",
        "colab_type": "text"
      },
      "source": [
        "Define a function for data generation. \n",
        "### Important: \n",
        "*realCodes* is the correct dataset of billing codes for a patient visit\n",
        "\n",
        "*newCodes* is the noisy data (with errors added at random to simulate human error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m6VoAZlfJGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getRecords(stamp,mistakeChance=1,recordsToGenerate=1000):\n",
        "    bodyParts = ['ankle', 'arch', 'arm', 'armpit', 'beard', 'breast', 'calf', 'cheek', 'chest', 'chin', 'earlobe', 'elbow', 'eyebrow', 'eyelash', 'eyelid', 'face', 'finger', 'forearm', 'forehead', 'gum', 'heel', 'hip', 'index finger', 'jaw', 'knee', 'knuckle', 'leg', 'lip', 'mouth', 'mustache', 'nail', 'neck', 'nostril', 'palm', 'pinkie', 'pupil', 'scalp', 'shin', 'shoulder', 'sideburns', 'thigh', 'throat', 'thumb', 'tongue', 'tooth', 'waist', 'wrist']\n",
        "    psychDisorders = ['Alcohol Addiction','Drug Addiction','Caffeine Addiction','Cannabis Addiction','Hallucinogen Addiction','Inhalant Addiction','Opioid Addiction','Sedative, Hypnotic, Anxiolytic Addiction','Stimulant Addiction','Tobacco Addiction','Gambling Addiction','Agoraphobia','Generalized Anxiety Disorder','Panic Disorder','Selective Mutism','Separation Anxiety Disorder','Social Anxiety Disorder','Specific Phobias','Bipolar Disorder','Cyclothymia','Other Bipolar Disorders','Major Depression','Dysthymia (now called Persistent Depressive Disorder)','Postpartum Depression','Premenstrual Dysphoric Disorder','Seasonal Affective Disorder','Depersonalization / Derealization Disorder','Dissociative Amnesia','Dissociative Fugue','Dissociative Identity Disorder','Other Dissociative Disorders','Anorexia Nervosa','Binge Eating Disorder','Bulimia Nervosa','Pica','Conduct Disorder','Intermittent Explosive Disorder','Kleptomania','Oppositional Defiant Disorder','Pyromania','Alzheimer’s Disease','Amnestic Disorder','Delerium','Huntington’s Disease','Neurocognitive Disorder (formerly called Dementia)','Parkinson’s Disease','Other Neurocognitive Disorders','Asperger’s Syndrome','Attention Deficit Hyperactivity Disorder','Autism Spectrum Disorder','Childhood Disintegrative Disorder','Childhood Onset Fluency Disorder','Dyslexia','Intellectual Development Disorder','Language Disorder','Learning Disorders','Retts Disorder','Tourettes Syndrome','Other Neurodevelopmental Disorders','Body Dysmorphic Disorder','Obsessive-Compulsive Disorder','Trichotillomania','Other Obsessive-Compulsive Disorders','Antisocial Personality Disorder','Avoidant Personality Disorder','Borderline Personality Disorder','Dependent Personality Disorder','Histrionic Personality Disorder','Narcissistic Personality Disorder','Obsessive-Compulsive Personality Disorder','Paranoid Personality Disorder','Schizoid Personality Disorder','Schizotypal Personality Disorder','Other Personality Disorders','Brief Psychotic Disorder','Delusional Disorder','Schizoaffective Disorder','Schizophrenia','Shared Psychotic Disorder','Other Psychotic Disorders','Breathing-Related Sleep Disorder','Circadian Rhythm Disorders','Hypersomnia','Insomnia','Narcolepsy','Nightmare Disorder','Non Rapid Eye Movement','REM Sleep Behavior Disorder','Restless Leg Syndrome','Sleep Arousal Disorders','Other Sleep Disorders','Conversion Disorder','Factitious Disorder','Hypochondriasis','Malingering','Munchausen Syndrome','Munchausen by Proxy','Somatization Disorder','Other Somatic Disorders','Acute Stress Disorder','Adjustment Disorder','Posttraumatic Stress Disorder','Reactive Attachment Disorder','Other Trauma Disorders']\n",
        "    vaccines = ['ACAM2000','ActHIB','Adacel','Afluria','AFLURIA QUADRIVALENT','Agriflu','BCG Vaccine','BEXSERO','Biothrax','Boostrix','Cervarix','Comvax','DAPTACEL','Engerix-B','FLUAD','Fluarix','Fluarix Quadrivalent','Flublok','Flublok Quadrivalent','Flucelvax','Flucelvax Quadrivalent','FluLaval','FluLaval Quadrivalent','FluMist','FluMist Quadrivalent','Fluvirin','Fluzone Quadrivalent','Fluzone, Fluzone High-Dose and Fluzone Intradermal','Gardasil','Gardasil 9','Havrix','HEPLISAV-B','Hiberix','Imovax','Infanrix','IPOL','Ixiaro','JE-Vax','KINRIX','M-M-R II','M-M-Vax','Menactra','MenHibrix','Menomune-A/C/Y/W-135','Menveo','Pediarix','PedvaxHIB','Pentacel','Pneumovax 23','Poliovax','Prevnar 13','ProQuad','Quadracel','RabAvert','Recombivax HB','ROTARIX','RotaTeq','SHINGRIX','TENIVAC','TICE BCG','TRUMENBA','Twinrix','TYPHIM Vi','VAQTA','Varivax','Vaxchora','Vivotif','YF-Vax','Zostavax']\n",
        "    f=open(py_base_dir+\"patientRecords\"+stamp+\".csv\", \"a+\")\n",
        "    f.write('visitNote|newCodes|realCodes\\n')\n",
        "    startTime=time.time()\n",
        "    for i in range(recordsToGenerate):\n",
        "        if i%100==0:\n",
        "            secs=max(1,int(time.time()-startTime))\n",
        "            lng=float(i)\n",
        "            print(\"rows=\",lng,\"%=\",100*lng/recordsToGenerate,\"s=\",secs,\"rec/s=\",lng/secs)\n",
        "\n",
        "        actionList=['bodyPart','psychDisorder','vaccine','hasCold','catAllergy','dogAllergy','lactoseIntolerant','looksPale']\n",
        "        patientName = names.get_full_name()\n",
        "        visitNote = 'The patient '+patientName+' was assessed in the clinic. '\n",
        "        realCodes = [1,0,0,0]\n",
        "        shuffle(actionList)\n",
        "        # print(actionList)\n",
        "        for item in actionList:\n",
        "            visitNoteAppend, newCode = nextVisitAction(item,bodyParts,psychDisorders,vaccines)\n",
        "            realCodes[newCode]=1\n",
        "            if len(visitNoteAppend)>2:\n",
        "                visitNote = visitNote + ' ' + str(visitNoteAppend) #.decode('utf-8')\n",
        "        newCodes=[0,0,0,0]\n",
        "        for index in range(len(realCodes)):\n",
        "            chance = (random.randint(1,100))\n",
        "            # Incorrect if chance == mistakeChance == 1\n",
        "            # Odds of a mistake are 1:100, i.e. 1%\n",
        "            if chance<=mistakeChance:\n",
        "                #swap codes\n",
        "                opposite =0\n",
        "                if realCodes[index] is 0:\n",
        "                    opposite=1\n",
        "                newCodes[index]=opposite\n",
        "            else:\n",
        "                #copy codes\n",
        "                newCodes[index]=realCodes[index]\n",
        "        f.write(str(visitNote)+'|'+ str(newCodes)+ '|'+str(realCodes)+\"\\n\")\n",
        "    f.close()  \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW6BNeOoftxA",
        "colab_type": "text"
      },
      "source": [
        "Generate 5000 patient treatment records including correct billing codes, and also a set of billing codes containing errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGL7WvXcfpvT",
        "colab_type": "code",
        "outputId": "9de227d2-5095-4ac7-8651-6c5647155a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "stamp=str(time.time())\n",
        "getRecords(stamp,1,5000)\n",
        "df=pd.read_csv(py_base_dir+\"patientRecords\"+stamp+\".csv\", delimiter=\"|\")\n",
        "\n",
        "#create dataframe\n",
        "store = pd.HDFStore(py_base_dir+'patientRecords_'+stamp+'.h5')\n",
        "store['patientRecords'] = df\n",
        "store.close()\n",
        "print('patientRecords_'+stamp+'.h5')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rows= 0.0 %= 0.0 s= 1 rec/s= 0.0\n",
            "rows= 100.0 %= 2.0 s= 1 rec/s= 100.0\n",
            "rows= 200.0 %= 4.0 s= 1 rec/s= 200.0\n",
            "rows= 300.0 %= 6.0 s= 1 rec/s= 300.0\n",
            "rows= 400.0 %= 8.0 s= 2 rec/s= 200.0\n",
            "rows= 500.0 %= 10.0 s= 3 rec/s= 166.66666666666666\n",
            "rows= 600.0 %= 12.0 s= 3 rec/s= 200.0\n",
            "rows= 700.0 %= 14.0 s= 4 rec/s= 175.0\n",
            "rows= 800.0 %= 16.0 s= 4 rec/s= 200.0\n",
            "rows= 900.0 %= 18.0 s= 5 rec/s= 180.0\n",
            "rows= 1000.0 %= 20.0 s= 5 rec/s= 200.0\n",
            "rows= 1100.0 %= 22.0 s= 6 rec/s= 183.33333333333334\n",
            "rows= 1200.0 %= 24.0 s= 6 rec/s= 200.0\n",
            "rows= 1300.0 %= 26.0 s= 7 rec/s= 185.71428571428572\n",
            "rows= 1400.0 %= 28.0 s= 7 rec/s= 200.0\n",
            "rows= 1500.0 %= 30.0 s= 8 rec/s= 187.5\n",
            "rows= 1600.0 %= 32.0 s= 8 rec/s= 200.0\n",
            "rows= 1700.0 %= 34.0 s= 9 rec/s= 188.88888888888889\n",
            "rows= 1800.0 %= 36.0 s= 9 rec/s= 200.0\n",
            "rows= 1900.0 %= 38.0 s= 10 rec/s= 190.0\n",
            "rows= 2000.0 %= 40.0 s= 10 rec/s= 200.0\n",
            "rows= 2100.0 %= 42.0 s= 11 rec/s= 190.9090909090909\n",
            "rows= 2200.0 %= 44.0 s= 11 rec/s= 200.0\n",
            "rows= 2300.0 %= 46.0 s= 12 rec/s= 191.66666666666666\n",
            "rows= 2400.0 %= 48.0 s= 12 rec/s= 200.0\n",
            "rows= 2500.0 %= 50.0 s= 13 rec/s= 192.30769230769232\n",
            "rows= 2600.0 %= 52.0 s= 13 rec/s= 200.0\n",
            "rows= 2700.0 %= 54.0 s= 14 rec/s= 192.85714285714286\n",
            "rows= 2800.0 %= 56.0 s= 14 rec/s= 200.0\n",
            "rows= 2900.0 %= 58.0 s= 15 rec/s= 193.33333333333334\n",
            "rows= 3000.0 %= 60.0 s= 15 rec/s= 200.0\n",
            "rows= 3100.0 %= 62.0 s= 16 rec/s= 193.75\n",
            "rows= 3200.0 %= 64.0 s= 16 rec/s= 200.0\n",
            "rows= 3300.0 %= 66.0 s= 17 rec/s= 194.11764705882354\n",
            "rows= 3400.0 %= 68.0 s= 17 rec/s= 200.0\n",
            "rows= 3500.0 %= 70.0 s= 18 rec/s= 194.44444444444446\n",
            "rows= 3600.0 %= 72.0 s= 18 rec/s= 200.0\n",
            "rows= 3700.0 %= 74.0 s= 19 rec/s= 194.73684210526315\n",
            "rows= 3800.0 %= 76.0 s= 20 rec/s= 190.0\n",
            "rows= 3900.0 %= 78.0 s= 20 rec/s= 195.0\n",
            "rows= 4000.0 %= 80.0 s= 21 rec/s= 190.47619047619048\n",
            "rows= 4100.0 %= 82.0 s= 21 rec/s= 195.23809523809524\n",
            "rows= 4200.0 %= 84.0 s= 22 rec/s= 190.9090909090909\n",
            "rows= 4300.0 %= 86.0 s= 23 rec/s= 186.95652173913044\n",
            "rows= 4400.0 %= 88.0 s= 23 rec/s= 191.30434782608697\n",
            "rows= 4500.0 %= 90.0 s= 24 rec/s= 187.5\n",
            "rows= 4600.0 %= 92.0 s= 24 rec/s= 191.66666666666666\n",
            "rows= 4700.0 %= 94.0 s= 25 rec/s= 188.0\n",
            "rows= 4800.0 %= 96.0 s= 25 rec/s= 192.0\n",
            "rows= 4900.0 %= 98.0 s= 26 rec/s= 188.46153846153845\n",
            "patientRecords_1574883190.656413.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>visitNote</th>\n",
              "      <th>newCodes</th>\n",
              "      <th>realCodes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The patient Joe Ford was assessed in the clini...</td>\n",
              "      <td>[1, 0, 1, 0]</td>\n",
              "      <td>[1, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The patient Linda Carter was assessed in the c...</td>\n",
              "      <td>[1, 1, 0, 0]</td>\n",
              "      <td>[1, 1, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The patient Juan Veile was assessed in the cli...</td>\n",
              "      <td>[1, 1, 0, 0]</td>\n",
              "      <td>[1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The patient Lisa Miller was assessed in the cl...</td>\n",
              "      <td>[1, 0, 0, 0]</td>\n",
              "      <td>[1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The patient Mark Wills was assessed in the cli...</td>\n",
              "      <td>[1, 0, 0, 0]</td>\n",
              "      <td>[1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           visitNote  ...     realCodes\n",
              "0  The patient Joe Ford was assessed in the clini...  ...  [1, 0, 1, 0]\n",
              "1  The patient Linda Carter was assessed in the c...  ...  [1, 1, 0, 0]\n",
              "2  The patient Juan Veile was assessed in the cli...  ...  [1, 0, 0, 0]\n",
              "3  The patient Lisa Miller was assessed in the cl...  ...  [1, 0, 0, 0]\n",
              "4  The patient Mark Wills was assessed in the cli...  ...  [1, 0, 0, 0]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHjDVOWOgQyF",
        "colab_type": "text"
      },
      "source": [
        "Load model vectors using one of spaCy's GloVe models. See more here: https://spacy.io/models/en#en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dq8gtrXf_Sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWXZ8Ut6hPrM",
        "colab_type": "text"
      },
      "source": [
        "Now prepare the dataset for training and testing using a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak_v54B9hHjE",
        "colab_type": "code",
        "outputId": "7c6c46bd-3d33-41ba-fb5a-ad8fb5726331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def makeArr(s):\n",
        "  return np.array(s[1:-1].split(\",\")).astype(np.int)\n",
        "\n",
        "x = df.copy(deep=True)\n",
        "\n",
        "# turn note text into a GloVe vector for each visit note.\n",
        "def toVectors(row):\n",
        "    vec = nlp(row[\"visitNote\"])\n",
        "    return [vec.vector]\n",
        "\n",
        "x[\"vec\"] = x.apply(toVectors, axis=1)\n",
        "print('x vector shape = ', x[\"vec\"].shape)\n",
        "\n",
        "the_x = []\n",
        "for i in range(len(x[\"vec\"])):\n",
        "    the_x.append(x[\"vec\"].iloc[i][0])\n",
        "the_x=np.array(the_x)\n",
        "print('x location shape = ', the_x.shape)\n",
        "y = df[\"realCodes\"].apply(makeArr)\n",
        "print('y (realCodes) shape = ', y.shape)\n",
        "the_y = []\n",
        "for i in range(len(y)):\n",
        "    the_y.append(y[i][:])\n",
        "the_y=np.array(the_y)\n",
        "\n",
        "print('y location shape = ', the_y.shape)\n",
        "print('y location 1:30 shape', the_y[0:30])\n",
        "#after testing and training do model.predict on df[\"newCodes\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x vector shape =  (5000,)\n",
            "x location shape =  (5000, 96)\n",
            "y (realCodes) shape =  (5000,)\n",
            "y location shape =  (5000, 4)\n",
            "y location 1:30 shape [[1 0 1 0]\n",
            " [1 1 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 1 0]\n",
            " [1 0 0 1]\n",
            " [1 0 1 0]\n",
            " [1 0 1 0]\n",
            " [1 1 0 0]\n",
            " [1 1 1 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 1 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 1 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 1 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj6ex95ehZw8",
        "colab_type": "code",
        "outputId": "f50d7ba7-94f1-47f7-8d01-2e4e09c4c5b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "yNew = df[\"newCodes\"].apply(makeArr)\n",
        "print(yNew.shape)\n",
        "the_yNew = []\n",
        "for i in range(len(yNew)):\n",
        "    the_yNew.append(yNew.iloc[i][:])\n",
        "    \n",
        "the_yNew=np.array(the_yNew)\n",
        "print(the_yNew.shape)\n",
        "print(the_yNew[0:30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000,)\n",
            "(5000, 4)\n",
            "[[1 0 1 0]\n",
            " [1 1 0 0]\n",
            " [1 1 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 1 0]\n",
            " [1 0 0 1]\n",
            " [1 0 1 0]\n",
            " [1 0 0 0]\n",
            " [1 1 0 0]\n",
            " [1 1 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 1 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 1 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 1 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIaVP4-nhktM",
        "colab_type": "text"
      },
      "source": [
        "Define the neural network, plug in the data, and launch the model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9XXSSMqhidc",
        "colab_type": "code",
        "outputId": "4d1e5055-ea33-45b8-e99b-5dbdef6c1817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "# %%time\n",
        "y=the_y\n",
        "x=the_x\n",
        "\n",
        "#test/train split\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Dropout, Activation\n",
        "from keras.losses import binary_crossentropy,sparse_categorical_crossentropy, mean_squared_error\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "\n",
        "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.20, random_state=42)\n",
        "\n",
        "y_train=np.hsplit(y_train, y_train.shape[1])\n",
        "y_test =np.hsplit(y_test, y_test.shape[1])\n",
        "\n",
        "print(x_train.shape, x_test.shape)\n",
        "for i in range(len(y_train)):\n",
        "  print(y_train[i].shape, y_test[i].shape)\n",
        "\n",
        "# This returns a tensor\n",
        "inputs = Input(shape=(x_train.shape[1],))\n",
        "layer1 = Dense(128, activation='relu')(inputs)\n",
        "# d1 = Dropout(.5)(layer1)\n",
        "d1=layer1\n",
        "for i in range(2):\n",
        "  d1=Dense(64, activation='relu')(d1)\n",
        "  # d1=Dropout(.5)(d1)\n",
        "\n",
        "outs=[]\n",
        "losses=[]\n",
        "for i in range(y.shape[1]):\n",
        "  outs.append(Dense(1, activation='sigmoid')(d1))\n",
        "  losses.append(binary_crossentropy)\n",
        "model = Model(inputs=inputs, outputs=outs)\n",
        "model.compile(optimizer=\"adam\", loss=losses, metrics=['accuracy'])\n",
        "# print(model.summary())\n",
        "\n",
        "\n",
        "!mkdir {base_dir}.log\n",
        "tbCallBack = TensorBoard(log_dir=py_base_dir+'/.log', #histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=512,\n",
        "                         write_images=True)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=100,  batch_size=512, callbacks=[tbCallBack])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard module is not an IPython extension.\n",
            "(4000, 96) (1000, 96)\n",
            "(4000, 1) (1000, 1)\n",
            "(4000, 1) (1000, 1)\n",
            "(4000, 1) (1000, 1)\n",
            "(4000, 1) (1000, 1)\n",
            "mkdir: cannot create directory ‘/content/gdrive/My Drive/AuditMap_workshop/.log’: File exists\n",
            "Epoch 1/100\n",
            "4000/4000 [==============================] - 0s 108us/step - loss: 2.0967 - dense_4_loss: 0.4014 - dense_5_loss: 0.4479 - dense_6_loss: 0.7225 - dense_7_loss: 0.5249 - dense_4_acc: 0.9630 - dense_5_acc: 0.8440 - dense_6_acc: 0.4645 - dense_7_acc: 0.9315\n",
            "Epoch 2/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 1.1729 - dense_4_loss: 0.0931 - dense_5_loss: 0.4065 - dense_6_loss: 0.4519 - dense_7_loss: 0.2213 - dense_4_acc: 1.0000 - dense_5_acc: 0.8440 - dense_6_acc: 0.7957 - dense_7_acc: 0.9575\n",
            "Epoch 3/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.8689 - dense_4_loss: 0.0123 - dense_5_loss: 0.3318 - dense_6_loss: 0.3497 - dense_7_loss: 0.1750 - dense_4_acc: 1.0000 - dense_5_acc: 0.8442 - dense_6_acc: 0.8055 - dense_7_acc: 0.9575\n",
            "Epoch 4/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.6790 - dense_4_loss: 0.0041 - dense_5_loss: 0.2298 - dense_6_loss: 0.2713 - dense_7_loss: 0.1739 - dense_4_acc: 1.0000 - dense_5_acc: 0.9010 - dense_6_acc: 0.8778 - dense_7_acc: 0.9575\n",
            "Epoch 5/100\n",
            "4000/4000 [==============================] - 0s 22us/step - loss: 0.5206 - dense_4_loss: 0.0022 - dense_5_loss: 0.1600 - dense_6_loss: 0.1933 - dense_7_loss: 0.1651 - dense_4_acc: 1.0000 - dense_5_acc: 0.9587 - dense_6_acc: 0.9423 - dense_7_acc: 0.9575\n",
            "Epoch 6/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.3932 - dense_4_loss: 0.0016 - dense_5_loss: 0.1041 - dense_6_loss: 0.1312 - dense_7_loss: 0.1562 - dense_4_acc: 1.0000 - dense_5_acc: 0.9783 - dense_6_acc: 0.9640 - dense_7_acc: 0.9575\n",
            "Epoch 7/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.2927 - dense_4_loss: 0.0013 - dense_5_loss: 0.0615 - dense_6_loss: 0.0833 - dense_7_loss: 0.1466 - dense_4_acc: 1.0000 - dense_5_acc: 0.9950 - dense_6_acc: 0.9810 - dense_7_acc: 0.9575\n",
            "Epoch 8/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.2211 - dense_4_loss: 8.5927e-04 - dense_5_loss: 0.0347 - dense_6_loss: 0.0530 - dense_7_loss: 0.1325 - dense_4_acc: 1.0000 - dense_5_acc: 0.9985 - dense_6_acc: 0.9932 - dense_7_acc: 0.9575\n",
            "Epoch 9/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.1740 - dense_4_loss: 6.7481e-04 - dense_5_loss: 0.0217 - dense_6_loss: 0.0346 - dense_7_loss: 0.1171 - dense_4_acc: 1.0000 - dense_5_acc: 0.9997 - dense_6_acc: 0.9990 - dense_7_acc: 0.9580\n",
            "Epoch 10/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 0.1436 - dense_4_loss: 6.1613e-04 - dense_5_loss: 0.0147 - dense_6_loss: 0.0242 - dense_7_loss: 0.1041 - dense_4_acc: 1.0000 - dense_5_acc: 0.9997 - dense_6_acc: 0.9992 - dense_7_acc: 0.9622\n",
            "Epoch 11/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.1217 - dense_4_loss: 4.9931e-04 - dense_5_loss: 0.0105 - dense_6_loss: 0.0177 - dense_7_loss: 0.0930 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9995 - dense_7_acc: 0.9653\n",
            "Epoch 12/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.1005 - dense_4_loss: 3.8980e-04 - dense_5_loss: 0.0082 - dense_6_loss: 0.0136 - dense_7_loss: 0.0784 - dense_4_acc: 1.0000 - dense_5_acc: 0.9997 - dense_6_acc: 0.9997 - dense_7_acc: 0.9723\n",
            "Epoch 13/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.0839 - dense_4_loss: 3.0332e-04 - dense_5_loss: 0.0066 - dense_6_loss: 0.0108 - dense_7_loss: 0.0662 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9758\n",
            "Epoch 14/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.0702 - dense_4_loss: 2.4573e-04 - dense_5_loss: 0.0054 - dense_6_loss: 0.0090 - dense_7_loss: 0.0555 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9773\n",
            "Epoch 15/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 0.0590 - dense_4_loss: 1.7135e-04 - dense_5_loss: 0.0047 - dense_6_loss: 0.0075 - dense_7_loss: 0.0467 - dense_4_acc: 1.0000 - dense_5_acc: 0.9997 - dense_6_acc: 0.9997 - dense_7_acc: 0.9843\n",
            "Epoch 16/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.0500 - dense_4_loss: 1.4071e-04 - dense_5_loss: 0.0039 - dense_6_loss: 0.0062 - dense_7_loss: 0.0397 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9848\n",
            "Epoch 17/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.0420 - dense_4_loss: 1.0943e-04 - dense_5_loss: 0.0035 - dense_6_loss: 0.0054 - dense_7_loss: 0.0330 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9887\n",
            "Epoch 18/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 0.0345 - dense_4_loss: 9.5270e-05 - dense_5_loss: 0.0031 - dense_6_loss: 0.0047 - dense_7_loss: 0.0266 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9937\n",
            "Epoch 19/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 0.0289 - dense_4_loss: 8.2826e-05 - dense_5_loss: 0.0028 - dense_6_loss: 0.0040 - dense_7_loss: 0.0220 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9935\n",
            "Epoch 20/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.0243 - dense_4_loss: 7.1057e-05 - dense_5_loss: 0.0026 - dense_6_loss: 0.0036 - dense_7_loss: 0.0181 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9955\n",
            "Epoch 21/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 0.0208 - dense_4_loss: 6.8952e-05 - dense_5_loss: 0.0023 - dense_6_loss: 0.0033 - dense_7_loss: 0.0151 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9962\n",
            "Epoch 22/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 0.0178 - dense_4_loss: 5.2375e-05 - dense_5_loss: 0.0021 - dense_6_loss: 0.0028 - dense_7_loss: 0.0128 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9977\n",
            "Epoch 23/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 0.0152 - dense_4_loss: 4.9950e-05 - dense_5_loss: 0.0019 - dense_6_loss: 0.0025 - dense_7_loss: 0.0107 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9977\n",
            "Epoch 24/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.0134 - dense_4_loss: 4.1590e-05 - dense_5_loss: 0.0018 - dense_6_loss: 0.0022 - dense_7_loss: 0.0093 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9982\n",
            "Epoch 25/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.0117 - dense_4_loss: 3.8641e-05 - dense_5_loss: 0.0017 - dense_6_loss: 0.0021 - dense_7_loss: 0.0079 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 0.9997 - dense_7_acc: 0.9995\n",
            "Epoch 26/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 0.0108 - dense_4_loss: 3.5291e-05 - dense_5_loss: 0.0015 - dense_6_loss: 0.0019 - dense_7_loss: 0.0074 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9992\n",
            "Epoch 27/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.0099 - dense_4_loss: 3.1545e-05 - dense_5_loss: 0.0015 - dense_6_loss: 0.0017 - dense_7_loss: 0.0067 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9987\n",
            "Epoch 28/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 0.0089 - dense_4_loss: 2.9531e-05 - dense_5_loss: 0.0015 - dense_6_loss: 0.0016 - dense_7_loss: 0.0058 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 29/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.0084 - dense_4_loss: 2.6994e-05 - dense_5_loss: 0.0014 - dense_6_loss: 0.0014 - dense_7_loss: 0.0056 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9987\n",
            "Epoch 30/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.0069 - dense_4_loss: 2.5646e-05 - dense_5_loss: 0.0012 - dense_6_loss: 0.0014 - dense_7_loss: 0.0043 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9995\n",
            "Epoch 31/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.0062 - dense_4_loss: 2.3330e-05 - dense_5_loss: 0.0012 - dense_6_loss: 0.0013 - dense_7_loss: 0.0037 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 32/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.0058 - dense_4_loss: 2.2014e-05 - dense_5_loss: 0.0011 - dense_6_loss: 0.0012 - dense_7_loss: 0.0035 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9995\n",
            "Epoch 33/100\n",
            "4000/4000 [==============================] - 0s 17us/step - loss: 0.0053 - dense_4_loss: 2.0957e-05 - dense_5_loss: 9.9544e-04 - dense_6_loss: 0.0011 - dense_7_loss: 0.0032 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 34/100\n",
            "4000/4000 [==============================] - 0s 17us/step - loss: 0.0049 - dense_4_loss: 1.9339e-05 - dense_5_loss: 9.4650e-04 - dense_6_loss: 0.0010 - dense_7_loss: 0.0029 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 35/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 0.0049 - dense_4_loss: 1.8480e-05 - dense_5_loss: 8.8320e-04 - dense_6_loss: 9.7077e-04 - dense_7_loss: 0.0030 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9995\n",
            "Epoch 36/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.0044 - dense_4_loss: 1.9094e-05 - dense_5_loss: 8.6355e-04 - dense_6_loss: 9.1019e-04 - dense_7_loss: 0.0026 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 37/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.0040 - dense_4_loss: 1.9358e-05 - dense_5_loss: 7.6805e-04 - dense_6_loss: 8.8175e-04 - dense_7_loss: 0.0023 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 38/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 0.0039 - dense_4_loss: 1.8454e-05 - dense_5_loss: 7.4210e-04 - dense_6_loss: 8.1564e-04 - dense_7_loss: 0.0023 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 39/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.0036 - dense_4_loss: 1.6823e-05 - dense_5_loss: 6.8838e-04 - dense_6_loss: 7.7720e-04 - dense_7_loss: 0.0021 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 40/100\n",
            "4000/4000 [==============================] - 0s 15us/step - loss: 0.0033 - dense_4_loss: 1.4776e-05 - dense_5_loss: 6.6863e-04 - dense_6_loss: 7.4860e-04 - dense_7_loss: 0.0018 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 41/100\n",
            "4000/4000 [==============================] - 0s 15us/step - loss: 0.0032 - dense_4_loss: 1.4790e-05 - dense_5_loss: 6.2726e-04 - dense_6_loss: 6.9064e-04 - dense_7_loss: 0.0018 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 42/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 0.0032 - dense_4_loss: 1.4134e-05 - dense_5_loss: 5.7672e-04 - dense_6_loss: 6.8734e-04 - dense_7_loss: 0.0020 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9995\n",
            "Epoch 43/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 0.0030 - dense_4_loss: 1.2463e-05 - dense_5_loss: 5.7012e-04 - dense_6_loss: 6.4422e-04 - dense_7_loss: 0.0018 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 44/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.0029 - dense_4_loss: 1.1807e-05 - dense_5_loss: 5.3467e-04 - dense_6_loss: 6.3129e-04 - dense_7_loss: 0.0017 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 45/100\n",
            "4000/4000 [==============================] - 0s 17us/step - loss: 0.0026 - dense_4_loss: 1.1358e-05 - dense_5_loss: 5.3025e-04 - dense_6_loss: 5.8180e-04 - dense_7_loss: 0.0015 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 46/100\n",
            "4000/4000 [==============================] - 0s 23us/step - loss: 0.0026 - dense_4_loss: 1.0314e-05 - dense_5_loss: 4.9954e-04 - dense_6_loss: 5.6822e-04 - dense_7_loss: 0.0015 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 47/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.0025 - dense_4_loss: 9.6757e-06 - dense_5_loss: 4.7094e-04 - dense_6_loss: 5.4169e-04 - dense_7_loss: 0.0015 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 48/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.0024 - dense_4_loss: 9.0944e-06 - dense_5_loss: 4.2827e-04 - dense_6_loss: 5.2975e-04 - dense_7_loss: 0.0014 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 49/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.0020 - dense_4_loss: 8.7140e-06 - dense_5_loss: 4.1993e-04 - dense_6_loss: 4.8534e-04 - dense_7_loss: 0.0011 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 50/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.0019 - dense_4_loss: 7.9986e-06 - dense_5_loss: 4.0484e-04 - dense_6_loss: 4.6701e-04 - dense_7_loss: 0.0010 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 51/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 0.0019 - dense_4_loss: 7.3079e-06 - dense_5_loss: 3.8155e-04 - dense_6_loss: 4.5476e-04 - dense_7_loss: 0.0010 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 52/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 0.0019 - dense_4_loss: 7.0754e-06 - dense_5_loss: 3.8037e-04 - dense_6_loss: 4.3713e-04 - dense_7_loss: 0.0011 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 53/100\n",
            "4000/4000 [==============================] - 0s 26us/step - loss: 0.0019 - dense_4_loss: 6.4267e-06 - dense_5_loss: 3.6375e-04 - dense_6_loss: 4.3461e-04 - dense_7_loss: 0.0011 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 54/100\n",
            "4000/4000 [==============================] - 0s 33us/step - loss: 0.0017 - dense_4_loss: 6.1859e-06 - dense_5_loss: 3.3754e-04 - dense_6_loss: 4.0318e-04 - dense_7_loss: 9.6376e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 0.9997\n",
            "Epoch 55/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 0.0015 - dense_4_loss: 5.7992e-06 - dense_5_loss: 3.2595e-04 - dense_6_loss: 4.0085e-04 - dense_7_loss: 8.1485e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 56/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.0016 - dense_4_loss: 5.4307e-06 - dense_5_loss: 3.1439e-04 - dense_6_loss: 3.8938e-04 - dense_7_loss: 9.1594e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 57/100\n",
            "4000/4000 [==============================] - 0s 23us/step - loss: 0.0015 - dense_4_loss: 5.1112e-06 - dense_5_loss: 3.1185e-04 - dense_6_loss: 3.7481e-04 - dense_7_loss: 7.7926e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 58/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.0015 - dense_4_loss: 4.8907e-06 - dense_5_loss: 3.0216e-04 - dense_6_loss: 3.6791e-04 - dense_7_loss: 7.8098e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 59/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.0014 - dense_4_loss: 4.5647e-06 - dense_5_loss: 2.8674e-04 - dense_6_loss: 3.3887e-04 - dense_7_loss: 7.3745e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 60/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.0013 - dense_4_loss: 4.4270e-06 - dense_5_loss: 2.7658e-04 - dense_6_loss: 3.3108e-04 - dense_7_loss: 7.1646e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 61/100\n",
            "4000/4000 [==============================] - 0s 15us/step - loss: 0.0012 - dense_4_loss: 4.1346e-06 - dense_5_loss: 2.6799e-04 - dense_6_loss: 3.1641e-04 - dense_7_loss: 6.5878e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 62/100\n",
            "4000/4000 [==============================] - 0s 23us/step - loss: 0.0012 - dense_4_loss: 3.7985e-06 - dense_5_loss: 2.5014e-04 - dense_6_loss: 3.0861e-04 - dense_7_loss: 6.4965e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 63/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.0012 - dense_4_loss: 3.7077e-06 - dense_5_loss: 2.4353e-04 - dense_6_loss: 2.9872e-04 - dense_7_loss: 6.0572e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 64/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 0.0011 - dense_4_loss: 3.5795e-06 - dense_5_loss: 2.4021e-04 - dense_6_loss: 2.8866e-04 - dense_7_loss: 6.0924e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 65/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 0.0011 - dense_4_loss: 3.3727e-06 - dense_5_loss: 2.3413e-04 - dense_6_loss: 2.8060e-04 - dense_7_loss: 6.1631e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 66/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.0011 - dense_4_loss: 3.0954e-06 - dense_5_loss: 2.1847e-04 - dense_6_loss: 2.7482e-04 - dense_7_loss: 5.8515e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 67/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 0.0010 - dense_4_loss: 3.0044e-06 - dense_5_loss: 2.1319e-04 - dense_6_loss: 2.6574e-04 - dense_7_loss: 5.4984e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 68/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 9.9087e-04 - dense_4_loss: 2.9108e-06 - dense_5_loss: 2.0894e-04 - dense_6_loss: 2.5877e-04 - dense_7_loss: 5.2024e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 69/100\n",
            "4000/4000 [==============================] - 0s 22us/step - loss: 9.5559e-04 - dense_4_loss: 2.7296e-06 - dense_5_loss: 2.0068e-04 - dense_6_loss: 2.5250e-04 - dense_7_loss: 4.9968e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 70/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 9.5150e-04 - dense_4_loss: 2.5630e-06 - dense_5_loss: 1.9368e-04 - dense_6_loss: 2.4313e-04 - dense_7_loss: 5.1213e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 71/100\n",
            "4000/4000 [==============================] - 0s 22us/step - loss: 8.9317e-04 - dense_4_loss: 2.5248e-06 - dense_5_loss: 1.8761e-04 - dense_6_loss: 2.3672e-04 - dense_7_loss: 4.6632e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 72/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 8.8491e-04 - dense_4_loss: 2.3870e-06 - dense_5_loss: 1.8488e-04 - dense_6_loss: 2.3013e-04 - dense_7_loss: 4.6751e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 73/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 8.6132e-04 - dense_4_loss: 2.2725e-06 - dense_5_loss: 1.8026e-04 - dense_6_loss: 2.2419e-04 - dense_7_loss: 4.5460e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 74/100\n",
            "4000/4000 [==============================] - 0s 22us/step - loss: 8.3743e-04 - dense_4_loss: 2.1737e-06 - dense_5_loss: 1.7670e-04 - dense_6_loss: 2.1938e-04 - dense_7_loss: 4.3917e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 75/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 8.1219e-04 - dense_4_loss: 2.0963e-06 - dense_5_loss: 1.6819e-04 - dense_6_loss: 2.1289e-04 - dense_7_loss: 4.2901e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 76/100\n",
            "4000/4000 [==============================] - 0s 22us/step - loss: 7.8434e-04 - dense_4_loss: 2.0499e-06 - dense_5_loss: 1.6230e-04 - dense_6_loss: 2.0747e-04 - dense_7_loss: 4.1252e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 77/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 7.6098e-04 - dense_4_loss: 1.9489e-06 - dense_5_loss: 1.5801e-04 - dense_6_loss: 2.0128e-04 - dense_7_loss: 3.9974e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 78/100\n",
            "4000/4000 [==============================] - 0s 17us/step - loss: 7.4349e-04 - dense_4_loss: 1.8523e-06 - dense_5_loss: 1.5668e-04 - dense_6_loss: 1.9737e-04 - dense_7_loss: 3.8760e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 79/100\n",
            "4000/4000 [==============================] - 0s 17us/step - loss: 7.2779e-04 - dense_4_loss: 1.7712e-06 - dense_5_loss: 1.5395e-04 - dense_6_loss: 1.9243e-04 - dense_7_loss: 3.7964e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 80/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 7.1283e-04 - dense_4_loss: 1.7307e-06 - dense_5_loss: 1.5135e-04 - dense_6_loss: 1.8803e-04 - dense_7_loss: 3.7171e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 81/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 6.9508e-04 - dense_4_loss: 1.6583e-06 - dense_5_loss: 1.4219e-04 - dense_6_loss: 1.8270e-04 - dense_7_loss: 3.6853e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 82/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 6.7904e-04 - dense_4_loss: 1.5944e-06 - dense_5_loss: 1.4029e-04 - dense_6_loss: 1.8037e-04 - dense_7_loss: 3.5680e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 83/100\n",
            "4000/4000 [==============================] - 0s 22us/step - loss: 6.6190e-04 - dense_4_loss: 1.5496e-06 - dense_5_loss: 1.3959e-04 - dense_6_loss: 1.7445e-04 - dense_7_loss: 3.4631e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 84/100\n",
            "4000/4000 [==============================] - 0s 22us/step - loss: 6.4234e-04 - dense_4_loss: 1.4713e-06 - dense_5_loss: 1.3306e-04 - dense_6_loss: 1.7113e-04 - dense_7_loss: 3.3668e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 85/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 6.5199e-04 - dense_4_loss: 1.3700e-06 - dense_5_loss: 1.2820e-04 - dense_6_loss: 1.6799e-04 - dense_7_loss: 3.5443e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 86/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 6.1110e-04 - dense_4_loss: 1.3887e-06 - dense_5_loss: 1.2499e-04 - dense_6_loss: 1.6374e-04 - dense_7_loss: 3.2098e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 87/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 6.3201e-04 - dense_4_loss: 1.3113e-06 - dense_5_loss: 1.2228e-04 - dense_6_loss: 1.6142e-04 - dense_7_loss: 3.4700e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 88/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 5.9090e-04 - dense_4_loss: 1.3023e-06 - dense_5_loss: 1.1989e-04 - dense_6_loss: 1.5735e-04 - dense_7_loss: 3.1236e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 89/100\n",
            "4000/4000 [==============================] - 0s 15us/step - loss: 6.0126e-04 - dense_4_loss: 1.2290e-06 - dense_5_loss: 1.1983e-04 - dense_6_loss: 1.5319e-04 - dense_7_loss: 3.2701e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 90/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 5.6771e-04 - dense_4_loss: 1.2057e-06 - dense_5_loss: 1.1235e-04 - dense_6_loss: 1.4877e-04 - dense_7_loss: 3.0538e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 91/100\n",
            "4000/4000 [==============================] - 0s 19us/step - loss: 5.4308e-04 - dense_4_loss: 1.1771e-06 - dense_5_loss: 1.1130e-04 - dense_6_loss: 1.4694e-04 - dense_7_loss: 2.8366e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 92/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 5.3620e-04 - dense_4_loss: 1.1451e-06 - dense_5_loss: 1.0817e-04 - dense_6_loss: 1.4395e-04 - dense_7_loss: 2.8294e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 93/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 5.2720e-04 - dense_4_loss: 1.0894e-06 - dense_5_loss: 1.0532e-04 - dense_6_loss: 1.4093e-04 - dense_7_loss: 2.7987e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 94/100\n",
            "4000/4000 [==============================] - 0s 17us/step - loss: 5.1646e-04 - dense_4_loss: 1.0800e-06 - dense_5_loss: 1.0378e-04 - dense_6_loss: 1.3793e-04 - dense_7_loss: 2.7367e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 95/100\n",
            "4000/4000 [==============================] - 0s 15us/step - loss: 4.8908e-04 - dense_4_loss: 1.0384e-06 - dense_5_loss: 1.0080e-04 - dense_6_loss: 1.3409e-04 - dense_7_loss: 2.5315e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 96/100\n",
            "4000/4000 [==============================] - 0s 16us/step - loss: 5.1165e-04 - dense_4_loss: 1.0271e-06 - dense_5_loss: 1.0531e-04 - dense_6_loss: 1.3200e-04 - dense_7_loss: 2.7332e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 97/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 4.7815e-04 - dense_4_loss: 9.6084e-07 - dense_5_loss: 9.6708e-05 - dense_6_loss: 1.2992e-04 - dense_7_loss: 2.5056e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 98/100\n",
            "4000/4000 [==============================] - 0s 20us/step - loss: 4.5374e-04 - dense_4_loss: 9.5540e-07 - dense_5_loss: 9.5780e-05 - dense_6_loss: 1.2579e-04 - dense_7_loss: 2.3121e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 99/100\n",
            "4000/4000 [==============================] - 0s 18us/step - loss: 4.4866e-04 - dense_4_loss: 9.4980e-07 - dense_5_loss: 9.3332e-05 - dense_6_loss: 1.2353e-04 - dense_7_loss: 2.3085e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n",
            "Epoch 100/100\n",
            "4000/4000 [==============================] - 0s 21us/step - loss: 4.4888e-04 - dense_4_loss: 8.7935e-07 - dense_5_loss: 9.0062e-05 - dense_6_loss: 1.2245e-04 - dense_7_loss: 2.3549e-04 - dense_4_acc: 1.0000 - dense_5_acc: 1.0000 - dense_6_acc: 1.0000 - dense_7_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4c637cb4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYnZ8DJXh7Zy",
        "colab_type": "text"
      },
      "source": [
        "Let's see how often the billing code corrections are wrong:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADEIsKK6hyNh",
        "colab_type": "code",
        "outputId": "680d6db7-53c5-488a-d07c-96ebee4ed4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "score = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(score)\n",
        "fails=0\n",
        "for i in range(y_test[0].shape[0]): #\n",
        "    y_true=np.array([y_test[0][i],y_test[1][i],y_test[2][i],y_test[3][i]]).flatten()\n",
        "    p_x=x_test[i]\n",
        "    p_x=p_x[np.newaxis,:]\n",
        "    prediction=model.predict(p_x)\n",
        "    #prints everything\n",
        "    #print(y_true,np.around(prediction).flatten().astype(np.int))\n",
        "    y_pred=np.around(prediction).flatten().astype(np.int)\n",
        "    if not np.array_equal(y_true,y_pred):\n",
        "      print(y_true,y_pred)\n",
        "      fails+=1\n",
        "print(fails,\"out of\",y_test[0].shape[0],\"predictions were wrong\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 122us/step\n",
            "[0.006845786179183051, 8.623011926829349e-07, 0.0060346907352359265, 0.00039148143856436945, 0.00041875168844126166, 1.0, 0.999, 1.0, 1.0]\n",
            "[1 1 0 1] [1 0 0 1]\n",
            "1 out of 1000 predictions were wrong\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQZ44sVwJY4E",
        "colab_type": "code",
        "outputId": "298891f4-60c5-4589-b8c2-1249cc4362a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorboard --logdir {base_dir}/.log"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hd3exHkG4rJ",
        "colab_type": "text"
      },
      "source": [
        "OK, that was pretty cool. However, don't expect this level of quality from real world datasets.... \n",
        "\n",
        "# Customizing a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuUB7oGMOIU-",
        "colab_type": "code",
        "outputId": "2a4b822b-ccb6-457b-e600-612b46f8fc6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "!pip3 install lemay-ai-sidecar"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lemay-ai-sidecar in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: tensorflow<1.14.*,>=1.11.* in /usr/local/lib/python3.6/dist-packages (from lemay-ai-sidecar) (1.13.2)\n",
            "Requirement already satisfied: wget>=3.2 in /usr/local/lib/python3.6/dist-packages (from lemay-ai-sidecar) (3.2)\n",
            "Requirement already satisfied: numpy>=1.16.* in /usr/local/lib/python3.6/dist-packages (from lemay-ai-sidecar) (1.17.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from lemay-ai-sidecar) (0.21.3)\n",
            "Requirement already satisfied: gensim>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from lemay-ai-sidecar) (3.8.1)\n",
            "Requirement already satisfied: pandas>=0.24.* in /usr/local/lib/python3.6/dist-packages (from lemay-ai-sidecar) (0.25.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from lemay-ai-sidecar) (1.3.2)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from lemay-ai-sidecar) (2.2.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (1.12.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (0.8.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (1.13.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->lemay-ai-sidecar) (0.14.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.8.0->lemay-ai-sidecar) (1.9.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.*->lemay-ai-sidecar) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.*->lemay-ai-sidecar) (2.6.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->lemay-ai-sidecar) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->lemay-ai-sidecar) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (0.16.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<1.14.*,>=1.11.*->lemay-ai-sidecar) (41.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (1.10.18)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (2.49.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (1.13.18)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (0.2.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->smart-open>=1.8.1->gensim>=3.8.0->lemay-ai-sidecar) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKKANu0aiCOc",
        "colab_type": "code",
        "outputId": "bae4c74f-1c06-455e-ceb8-418b1dda64b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!cd {base_dir} && mkdir sidecar\n",
        "!cd {base_dir}/sidecar && git clone https://github.com/lemay-ai/sidecar.git\n",
        "!cd {base_dir}/sidecar/sidecar && ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘sidecar’: File exists\n",
            "fatal: destination path 'sidecar' already exists and is not an empty directory.\n",
            "total 11671\n",
            "-rw------- 1 root root     8683 Nov 27 19:00 CWS_gen_mp.py\n",
            "-rw------- 1 root root 11128365 Nov 27 19:00 dataset.csv\n",
            "drwx------ 2 root root     4096 Nov 27 19:00 images\n",
            "drwx------ 2 root root     4096 Nov 27 19:00 lemay_ai_sidecar\n",
            "-rw------- 1 root root    35149 Nov 27 19:00 LICENSE\n",
            "-rw------- 1 root root   764859 Nov 27 19:00 notebook_showing_steps.ipynb\n",
            "-rw------- 1 root root     2936 Nov 27 19:00 README.md\n",
            "-rw------- 1 root root      405 Nov 27 19:00 setup.py\n",
            "-rw------- 1 root root      965 Nov 27 19:00 test.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLXymKyvPR1w",
        "colab_type": "code",
        "outputId": "cacc9567-15ac-4763-915d-10d0ba318ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!head -n 20 {base_dir}/sidecar/sidecar/dataset.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ",[LocalizedFileNames],body,tags\n",
            "0,,<p>How i can convert word file (.docx &amp; doc ) to .pdf in c# without using SaveAs() or Save method ? or without uploading on server?</p>,c#\n",
            "1,,\"<p>I essentially have the following:</p>\n",
            "\n",
            "<pre><code>    int? myVal = null;\n",
            "    myVal |= 1;\n",
            "    bool stillNull = myVal == null; //returns true\n",
            "</code></pre>\n",
            "\n",
            "<p>Why does this behave this way?  My understanding of bitwise operator/operand behavior is not terribly strong, and I could not find a reason that it wouldn't be treated as a simple assignment in this case.</p>\",c#\n",
            "2,,\"<p>I have a variable which I am populating with records from my database. I then will display this list on a view as a Drop down box. However, it fails once it reaches the drop down. </p>\n",
            "\n",
            "<p>Controller:</p>\n",
            "\n",
            "<pre><code>  public ActionResult Review() {\n",
            "            var reviews = reviewRepo.GetAllReviews();\n",
            "\n",
            "            var clients = clientRepo.Clients();    \n",
            "\n",
            "            List&lt;SelectListItem&gt; items = new SelectList(clients, \"\"ClientID\"\", \"\"ClientName\"\").ToList();\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb4cUT5wPgko",
        "colab_type": "code",
        "outputId": "944faaf1-383b-412e-fe92-5d583bc76b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(py_base_dir+\"/sidecar/sidecar/dataset.csv\",index_col=0)\n",
        "df=df.sample(frac=1.0)\n",
        "df.head(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>[LocalizedFileNames]</th>\n",
              "      <th>body</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;Having some problems with a javascript code...</td>\n",
              "      <td>php</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;I've got a large data set spanning many yea...</td>\n",
              "      <td>r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;I have a dataset that I loaded into R using...</td>\n",
              "      <td>r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;Interrupting the program below with Ctrl + ...</td>\n",
              "      <td>perl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;I am trying to read data from MySQL and sho...</td>\n",
              "      <td>vb.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;I'm new to python, and I'm having problems ...</td>\n",
              "      <td>python</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>786</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;This query takes about a minute to give res...</td>\n",
              "      <td>sql</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;I'm getting list of strings from a method a...</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;So I know how to increment. I have the foll...</td>\n",
              "      <td>vb.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;pre&gt;&lt;code&gt;  import requests \\n        def pos...</td>\n",
              "      <td>python</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     [LocalizedFileNames]  ...    tags\n",
              "372                   NaN  ...     php\n",
              "47                    NaN  ...       r\n",
              "958                   NaN  ...       r\n",
              "304                   NaN  ...    perl\n",
              "319                   NaN  ...  vb.net\n",
              "970                   NaN  ...  python\n",
              "786                   NaN  ...     sql\n",
              "993                   NaN  ...      c#\n",
              "547                   NaN  ...  vb.net\n",
              "756                   NaN  ...  python\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoMJiGloQH9c",
        "colab_type": "code",
        "outputId": "92ac8ea1-b038-4f6b-9109-fc17ebe0815c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "display(df[\"tags\"].value_counts())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "javascript    1000\n",
              "php           1000\n",
              "c++           1000\n",
              "r             1000\n",
              "c#            1000\n",
              "sql           1000\n",
              "python        1000\n",
              "perl          1000\n",
              "vb.net        1000\n",
              "java          1000\n",
              "Name: tags, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1ROoyVqQhqC",
        "colab_type": "code",
        "outputId": "8f4cdba4-408a-4191-e6c3-93246e7898e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "len_train=0\n",
        "len_test=0\n",
        "with open(py_base_dir+'/sidecar/model_text.test', 'w') as testFile:\n",
        "  with open(py_base_dir+'/sidecar/model_text.train', 'w') as trainFile:\n",
        "    for index,row in df.iterrows():\n",
        "      line_text=\"__label__\"+str(row['tags'])+\" \"+str(row['body']).replace(\"\\n\",\" \")+\"\\n\"\n",
        "      try:\n",
        "        if int(index)%10==0:\n",
        "          testFile.write(line_text)\n",
        "          len_test+=1\n",
        "        else:\n",
        "          trainFile.write(line_text)\n",
        "          len_train+=1\n",
        "      except:\n",
        "        print(\"nope\")\n",
        "trainFile.close()\n",
        "testFile.close()\n",
        "print(len_train,len_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nope\n",
            "9000 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJNyOuHsRwKr",
        "colab_type": "code",
        "outputId": "a481b35b-15d6-4e43-9be1-7d2d30fa457c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!head -n 10 {base_dir}/sidecar/model_text.train\n",
        "!head -n 10 {base_dir}/sidecar/model_text.test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__label__c++ <p>I'm trying to remove an element from a vector. But I think, I have a specific problem:</p>  <p><strong>My data from file of reading:</strong></p>  <p><em>move ctrl+a,F3</em></p>  <p><em>copy ctrl+v,shift+v</em></p>  <p><em>search F3,F4</em></p>  <ol> <li>trying to read from a file</li> <li>input a word ,which I want to earse(e.x.: move)</li> <li><p>And the problem is that, I need to input ONLY ONE (like in an example) word, and earse the whole string of commands(<em>move ctrl+a,F3</em>).  </p>  <p>What I need is just to find a string by one word. But in code below I can't do this,help,please solving a problem. In a code below, all I can is find only one word, if in a file there is only one word, not like (<em>move ctrl+a,F3</em>), but if a string consists of several words.. It can't find.</p>  <pre><code> std::vector&lt;std::string&gt; HotMap::remove_element(std::vector&lt;std::string&gt; MyVector){ //remove a element ,reading a vector of cmds, then deletting one chosen of them std::string delete_str; std::vector&lt;std::string&gt;::iterator iter_vec;  std::cout &lt;&lt; \"Enter an element,which you want to delete: \" &lt;&lt; std::endl; std::cin &gt;&gt; delete_str; for (auto &amp; c : delete_str) c = tolower(c);                                         //For the standard for the right searching,all letters are in lower registr   //Here, iter_ve is my vector(vec_get_smd) it swiches to algorithm  //std::ind that look for a cmd command ,which user is trying to input iter_vec = std::find(MyVector.begin(), MyVector.end(), delete_str);  if(iter_vec!= MyVector.end())                                                       //Check if is there an element that yoy want earse  {     std::cout &lt;&lt; \"Found a element and earsed it!\" &lt;&lt; std::endl;     MyVector.erase(iter_vec); } else {     std::cout &lt;&lt; \"The element wasn't found!\" &lt;&lt; std::endl; }  for (iter_vec = MyVector.begin(); iter_vec != MyVector.end(); ++iter_vec)           //Just for myself, showing results {     std::cout &lt;&lt; *iter_vec &lt;&lt; std::endl; }  return std::vector&lt;std::string&gt;(MyVector); } </code></pre>  <p>My results:</p></li> </ol>  <p><code>read KEYS from a file TWO,below</code></p>  <p><code>move,fsf,fsdf,aaa</code></p>  <p><code>copy</code></p>  <p><code>search</code></p>  <p><code>Enter an element,which you want to delete:</code> <code>move</code></p>  <p><code>The element wasn't found!</code></p>\n",
            "__label__perl <p>I have a problem about arithmetic expression in Perl.</p>  <p>I have already written the code but I couldn't fill inside of eval function.</p>  <p>Example:</p>  <pre><code>&gt;2+4  6 </code></pre>  <p>Another example:</p>  <pre><code>&gt;8-2*2  4 </code></pre>  <p>This is my program</p>  <pre><code>#!/usr/bin/perl    print \"&gt;\";  while (&lt;&gt;) {      eval(---------);     print \"\\n&gt;\";  } </code></pre>\n",
            "__label__r <p>I try to use foreach and parallel R loops for the second time and I get some difficulties.</p>  <p>Each iteration of my loop will create a matrix with a a different number of rows. For example:</p>  <pre><code>for (i in 1:66) {   #Calculation   ....     Mat &lt;- maxtrix(nrow=i, ncol=51863)   #Calculation   ...    save(Mat, file=paste(\"Mat_\",i, sep=\"\")) } </code></pre>  <p>So with my loop I will have 66 different matrix saved. I need these different 66 files but the code takes a long time so I try to use foreach:</p>  <pre><code>Mat &lt;- foreach(i=1:66) %dopar% {   #Calculation   ....     Mat &lt;- maxtrix(nrow=i, ncol=51863)   #Calculation   ... }  save? </code></pre>  <p>And here I don't know what to do because I know I can't save my matrix in my parallel loop but I don't want only one file at the end of the loop. Should I create a list (for foreach) and do another loop to save each element of my list?</p>  <p>Thanks</p>\n",
            "__label__sql <p>I learned that I can index a column like this : </p>  <pre><code>CREATE INDEX idx_lastname ON Persons (LastName); </code></pre>  <p>The source also says that I can index a combination of indexes like this : </p>  <pre><code>CREATE INDEX idx_pname ON Persons (LastName, FirstName); </code></pre>  <p>I want to know what happens if I add <code>FirstName</code>? In what way will the searching in the db be faster? What will happen if I index <code>FirstName</code> and <code>LastName</code> seperately? Thanks.</p>\n",
            "__label__perl <p>What is wrong with this code please?</p>  <p>I get an error: <code>Useless use of log in void context at ./test.pl line 12.</code></p>  <pre><code>#!/usr/bin/perl  use strict; use warnings;  log();  sub log {     print \"Test\";     return; } </code></pre>\n",
            "__label__c# <p>HI,</p>  <p>I am trying to convert the following vba code to c# and i am facing some errors.Hope someone can help me</p>  <p>vba code</p>  <pre><code>    Open \"C:\\testfile.txt\" For Input As #1         varii = \"\"         Do While Not EOF(1)         Line Input #1, strField         varii = varii &amp; \",\" &amp; strField         Loop         Close #1         astrFields = Split(varii, \",\")      For intIx = 1 To UBound(astrFields)     counter = 0     i = i + 1      Dim cn As New ADODB.Connection, cn2 As New ADODB.Connection     Dim rs As ADODB.Recordset     Dim connString As String     Dim SelectFieldName     Set cn = CurrentProject.Connection     SelectFieldName = astrFields(intIx)     Set rs = cn.OpenSchema(adSchemaColumns, _     Array(Empty, Empty, Empty, SelectFieldName))          If Left(rs!Table_Name, 4) &lt;&gt; \"MSys\" And Left(rs!Table_Name, 4) &lt;&gt; \"Abfr     \" Then  strSql = \"SELECT t.[\" &amp; astrFields(intIx) &amp; \"], t.fall from [\" &amp; rs!Tab     le_Name &amp; \"] t Inner Join 01UMWELT on t.fall = [01UMWELT].fall \"             End If     Set rs3 = CurrentDb.OpenRecordset(strSql)     Do While Not rs3.EOF     With rs3     feedbackmsg = \"Processing \" &amp; rs!Table_Name &amp; \" Record no : \"     &amp; .Fields(1)     SysCmd acSysCmdSetStatus, feedbackmsg     varii = Nz(.Fields(astrFields(intIx)), \"NullValue\")     If varii = \"NullValue\" Then     Call .Edit     .Fields(astrFields(intIx)) = 888     Call .Update </code></pre>  <p>THis is the c# code so far i have coded</p>  <pre><code>FileInfo theSourceFile = new FileInfo(\"C:\\\\csharp\\\\testfile.txt\");             StreamReader reader = theSourceFile.OpenText();             varii = \"\";              do             {                 text = reader.ReadLine();                 varii = varii + \",\" + reader.ReadLine();                 //Console.WriteLine(text);               } while (text != null);                string[] split = varii.Split(new Char[] {' '});              foreach (string s in split)             {                  if (s.Trim() != \"\")                     Console.WriteLine(s);             }                       int temp = split.GetUpperBound(1);                     for (intix = 1; intix &lt;= temp; intix++)                     {                         counter = 0;                          i++;                         ADODB.Connection cn = new ADODB.Connection();                         ADODB.Connection cn2 = new ADODB.Connection();                         ADODB.Recordset rs;                         object selectfieldname;                          //ConnectionClass conDatabase = new ADODB.Connection();                          cn.ConnectionString = \"Provider=Microsoft.Jet.OLEDB.4.0;\" +                                               \"Data Source='C:\\\\csharp\\\\accident_database.mdb';\";                          rs = cn.OpenSchema(ADODB.SchemaEnum.adSchemaTables, Missing.Value, Missing.Value);                           if (Microsoft.Vbe.Left(rs.Fields(\"Table_Name\").Value, 4) != \"MSys\" &amp;&amp; +                             Microsoft.Vbe.Left(rs.Fields(\"Table_Name\").Value, 4) != \"Abfr\")                         {                             strsql = \"SELECT t.[\" + split(intix) + \"],t.fall from [\" + rs.Fields(\"Table_Name\").Value + \"]\" +                                     \"t Inner join 01umwelt on t.fall = [01umwelt].fall\";                          }                         rs3 = CurrentDB.OpenRecordset(strsql);                          while (!rs3.EOF)                         {                              feedbackmsg = \"Processing\" + rs.Fields(\"Table_Name\").Value + \"Record no:\" + rs3.Fields(1).Value;                             SysCmd acSysCmdSetStatus, feecbackmsg;                             varii = Nz(rs3.Fields(astrfields(intix)).Value, \"NullValue\");                              if (varii == \"NullValue\")                             {                                 rs3.Edit();                                 rs3.Fields[astrfields(intix)].Value = 888;                                 rs3.Update(); </code></pre>  <p>I am recieving errors in the if statement </p>  <pre><code>if (Microsoft.Vbe.Left(rs.Fields(\"Table_Name\").Value, 4) != \"MSys\" &amp;&amp; +             Microsoft.Vbe.Left(rs.Fields(\"Table_Name\").Value, 4) != \"Abfr\") </code></pre>  <p>here Vbe.left is not getting accepted</p>  <p>and secondly in the sql statement following this i get an error split is an variablebut used like method.</p>  <p>Thanks</p>\n",
            "__label__c++ <p>How do I convert a <code>long</code> to a <code>string</code> in C++?</p>\n",
            "__label__vb.net <p>In <code>Tester.txt</code> it simply says \"Hello\".</p>  <p>I now run this:</p>  <pre><code>Sub Main()     Dim fileName As String     Dim aFileStream As FileStream     Dim aWriter As StreamWriter     fileName = \"...\\Tester.txt\"     Try         aFileStream = New FileStream(fileName, FileMode.OpenOrCreate)         aWriter = New StreamWriter(aFileStream)         aWriter.WriteLine(\"World\")     Catch ex As Exception         MessageBox.Show(\"found this {0}\", ex.Message)     Finally         If aWriter IsNot Nothing Then             aWriter.Close()         End If         If aFileStream IsNot Nothing Then             aFileStream.Close()         End If     End Try  End Sub </code></pre>  <p>Now \"Hello\" has been over-written with \"World\".</p>  <p>I'd like to preserve the existing text in the file so I thought I could read that text into a variable first and then write the new stuff:</p>  <pre><code>Sub Main()     Dim fileName As String     Dim aFileStream As FileStream     Dim aWriter As StreamWriter     Dim aReader As StreamReader     fileName = \"...\\Tester.txt\"     Try         aFileStream = New FileStream(fileName, FileMode.OpenOrCreate)          aReader = New StreamReader(aFileStream)         Dim myContent As String = aReader.ReadToEnd()          aWriter = New StreamWriter(aFileStream)         aWriter.WriteLine(\"World\")      Catch ex As Exception         MessageBox.Show(\"found this {0}\", ex.Message)     Finally         If aWriter IsNot Nothing Then             aWriter.Close()         End If         If aReader IsNot Nothing Then             aReader.Close()         End If         If aFileStream IsNot Nothing Then             aFileStream.Close()         End If     End Try  End Sub </code></pre>  <p>Is this the standard way of preserving the text already in the file?<br> Can I preserve the existing information without using an instance of <code>StreamReader</code>  ?</p>\n",
            "__label__sql <p>here is the schema:</p>  <p><a href=\"http://img30.imageshack.us/img30/3324/123vk.jpg\" rel=\"nofollow noreferrer\">alt text http://img30.imageshack.us/img30/3324/123vk.jpg</a></p>  <p>here is the question:</p>  <p>Point out the battles in which at least three ships from the same country took part.</p>  <p>here is my answer:</p>  <pre><code>  SELECT battles.name      FROM battles,          outcomes,          ships,          classes     WHERE outcomes.ship = ships.name  GROUP BY battles.name    HAVING COUNT(classes.country) &gt;= 3; </code></pre>  <p>Can you please tell me why it is wrong and help me to correct it!</p>\n",
            "__label__c# <p>I found this: <a href=\"http://msdn.microsoft.com/en-us/library/vslangproj80.reference3%28VS.80%29.aspx\" rel=\"nofollow noreferrer\">http://msdn.microsoft.com/en-us/library/vslangproj80.reference3%28VS.80%29.aspx</a></p>  <p>what I have in mind is that many of the references that we add to our project are on a network drive and there are TON of them. Adding references to the project by right clicking on the References in the porject and choosing add reference is a pain.</p>  <p>so I was wondering if I can take advantage of something like what I posted the link to it and have a small program,add-in,macro, etc! that we can give it a list of the references that I want and it will add them to the project.</p>\n",
            "__label__sql <p>I have a stored proc (called sprocGetArticles)  which returns a list of articles from the articles table. This stored proc does not have any parameters.</p>  <p>Users can leave comments for each article and I store these comments in the comments table linked by the article id.</p>  <p>Is there any way I can do a comment count for each articleid in the returned list from within the sprocGetArticles stored procedure so I only have to make one call to the database? </p>  <p>My problem is that i need the article id to do the count which I seem unable to declare.</p>  <p>Is this the best approach anyway?</p>\n",
            "__label__r <p>I import data from a CSV file like such</p>  <pre><code>x &lt;- c(read.csv(file=\"FX.csv\", sep=\";\", header = FALSE, dec=\",\")) sapply(x, typeof) </code></pre>  <p>which returns \"double\"</p>  <p>I am trying to get the mean of these numbers but I get the following</p>  <pre><code>mean(x) </code></pre>  <p>Warning message: In mean.default(x) : argument is not numeric or logical: returning NA</p>  <p>which makes sense since the given data type is double can someone please help me to get the mean of decimals data this is what my data looks like</p>  <pre><code>x     $V1      [1]  0.01  0.00  0.02  0.00  0.01  0.04 -0.02  0.00  0.01  0.00  0.00  0.01 -0.01 -0.02  0.01  0.00  0.00 -0.02 -0.01  0.00     [21]  0.01  0.01 -0.03  0.00  0.03  0.00 -0.02  0.00  0.00 -0.01  0.00 -0.01 -0.02  0.00  0.00 -0.02  0.03  0.00  0.01  0.00     [41] -0.01 -0.01  0.00  0.00 -0.02  0.00  0.00 -0.01  0.00  0.01  0.04  0.01 -0.05  0.00  0.00  0.00  0.02  0.00 -0.04 -0.01     [61]  0.01 -0.01  0.02  0.02 </code></pre>  <p>Here is the edit as was suggested with the results</p>  <pre><code>sapply(x, typeof)       V1  \"double\"  str(x) 'data.frame':   64 obs. of  1 variable:  $ V1: num  0.01 0 0.02 0 0.01 0.04 -0.02 0 0.01 0 ...  &gt; sapply(x, class)        V1  \"numeric\"  </code></pre>\n",
            "__label__java <p>I didn't understand why date2 printing <code>04/10/2016</code>,</p>  <p>1) i didn't call method which has RETURN<br>  2) there are two methods, with same Parameter but diff output</p>  <pre><code>MyDate date2 = new MyDate(4,10,2008);         System.out.println(date2);  ---------------------------------------- //MyDate class public class MyDate{   public int day;  public int month;  public int year;   public MyDate(){ }  //Constructor that takes 3 arguments public MyDate(int m, int d, int y){ setDate(m, d, y); } //Methods  public String toString1(){ return day + \"/\" + month + \"/\" + year; }  public String toString(){ return month + \"/\" + day + \"/\" + year; }  public void setDate(int m, int d, int y){ day = d; year = y; month = m; } } </code></pre>\n",
            "__label__c++ <p>I'm working on modifying Firaxis' Civilization 4 core game DLL. The host application is built using VC7, hence the constraint (source not provided for the host EXE).</p>  <p>I've been working on rewriting a large chunk of the code (focusing on low-hanging performance issues &amp; memory leaks). I recently ran into an internal compiler error when trying to mod the code to use an array class instead of dynamically allocated 2-d arrays, I was going to use matrices from the boost lib (Civ4 is already using boost, so why not?).</p>  <p>Basically, the issue comes down to: if I include \"boost/numeric/ublas/matrix.hpp\", I run into an internal compiler error C1204.</p>  <p>MSDN has this to say: <a href=\"http://msdn.microsoft.com/en-us/library/aa984672(VS.71).aspx\" rel=\"nofollow noreferrer\">MSDN C1204</a> KB has this to say: <a href=\"http://support.microsoft.com/kb/883655\" rel=\"nofollow noreferrer\">KB 883655</a></p>  <p>So, I'm curious, is it possible to solve this error without a KB/SP being applied and dramatically reducing the complexity of the code?</p>  <p>Additionally, as VC7 is no longer \"supported\", does anyone have a valid (supported) link for a VC7 service pack?</p>  <p><strong>Update:</strong> I do not have VS2003 installed; I only have the VS2003 toolkit (i.e. the freely downloaded compiler &amp; SDK, not the full IDE).</p>\n",
            "__label__java <p>I am working on a web application wherein I am recording sound from my microphone using a <a href=\"http://www.sajithmr.me/jrecorder/example1.html\" rel=\"nofollow\">flash plugin</a>. After the recording I can upload the recorded file to the server. Plz see the code below:</p>  <pre><code>            // Get the input stream             InputStream is = request.getInputStream();             InputStream bufferedIn = new BufferedInputStream(is);             AudioInputStream ais = AudioSystem.getAudioInputStream(bufferedIn);              // Declare the new format to convert to             AudioFormat audioFormat =  new AudioFormat(sampleRate, sampleSizeInBits, channels, signed, bigEndian);              // Convert the format and return the new audio input stream             ais = AudioSystem.getAudioInputStream(audioFormat, ais); </code></pre>  <p>Now, after this conversion I want to save the audio data from ais into a buffer and upload it to DB.</p>  <p>How do I do that? Thanks!! :)</p>\n",
            "__label__c# <p>I make an application that it get an image from net and mix it with an image in My Hard and generate new image. My Problem is increasing new photo size after this process.for example if main photo is 50X50 pixel with 50kb after this Operations new image is same 50X50 pixel but it new size is 400kb!</p>  <p>Please help me to resolve this awful problem.<br> I used THIS method to mix this image:</p>  <pre><code>HttpWebRequest MakeRequest = (HttpWebRequest)HttpWebRequest.Create(ImageURL); HttpWebResponse Respnse = (HttpWebResponse)MakeRequest.GetResponse(); Stream Streaming = Respnse.GetResponseStream(); Image ImageNews = Image.FromStream(Streaming);  imgToResize = ImageNews;   Image Logo = Image.FromFile(\"d:\\\\logonews.jpg\");  Bitmap NewsMainImage = new Bitmap(ImageNews.Width, ImageNews.Height); Graphics makeImage = Graphics.FromImage(NewsMainImage); makeImage.DrawImage(ImageNews,                       new Rectangle(new Point(), ImageNews.Size),                       new Rectangle(new Point(), ImageNews.Size),                       GraphicsUnit.Pixel);   makeImage.DrawImage(Logo,                      new Rectangle(new Point(0, ImageNews.Height - Logo.Height),                                     Logo.Size),                       new Rectangle(new Point(),                                    Logo.Size),                       GraphicsUnit.Pixel); </code></pre>\n",
            "__label__python <p>It's a really simple piece of code but it somehow never enters the if condition. I've printed all the values in the condition separately and I can see that they are the same in the console, but clearly Python is evaluating it as false.</p>  <pre><code>data = serializer.data print data socialid = data['socialid'] accesstoken = data['accesstoken'] graph = GraphAPI(accesstoken) print graph.get('me?fields=id')['id'] print socialid print graph.get('app/')['id'] print appid if (graph.get('me?fields=id')['id'] == socialid) and (graph.get('app/')['id'] == appid):     print \"hello\"  </code></pre>  <p>Am I missing something here? Does print ignore some extra characters (unicode.. something?)</p>\n",
            "__label__c# <p>I need help with a program that'll count the maximum number of 0's between two 1's in a given binary number in c#. For example 1100101(binary for 101) the maximum number of 0's between two 1's are 2. Any help?</p>  <p>This the code for counting the 0's in the string but not the 0's between 1's</p>  <pre><code>string bin = \"\"; int max = 0; for (int i = rem.Length - 1; i &gt;= 0; i--) {      if (rem[i] == '0')       {         bin = bin + rem[i];         c++;      }          else      {         bin = bin + rem[i];      } }  </code></pre>\n",
            "__label__r <p><strong>The Short</strong></p>  <p>I have </p>  <pre><code>X &lt;- data.frame(Animal = c(\"Ant\", \"Cat\", \"Dog\", \"Ant\", \"Dog\", \"Ant\", \"Ant\")) </code></pre>  <p>and I want to create add a column <code>freq</code> to <code>X</code> such that</p>  <pre><code>&gt; X   Animal Freq 1    Ant    4 2    Cat    1 3    Dog    2 4    Ant    4 5    Dog    2 6    Ant    4 7    Ant    4 </code></pre>  <p><strong>The Long</strong></p>  <p>I have </p>  <pre><code>&gt; X &lt;- data.frame(Animal = c(\"Ant\", \"Cat\", \"Dog\", \"Ant\", \"Dog\", \"Ant\", \"Ant\")) &gt; X   Animal 1    Ant 2    Cat 3    Dog 4    Ant 5    Dog 6    Ant 7    Ant </code></pre>  <p>I know that</p>  <pre><code>&gt; table(X) X Ant Cat Dog    4   1   2  </code></pre>  <p>Or</p>  <pre><code>&gt; count(X)   Animal freq 1    Ant    4 2    Cat    1 3    Dog    2 </code></pre>  <p>and that </p>  <pre><code>&gt; subset(count(X), Animal == \"Ant\")$freq [1] 4 </code></pre>  <p>and even that</p>  <pre><code>&gt; subset(count(X), Animal == X[1,1])$freq [1] 4 &gt; subset(count(X), Animal == X[2,1])$freq [1] 1 </code></pre>  <p>but I'm struggling to put all together to add a column <code>freq</code> to <code>X</code> such that</p>  <pre><code>&gt; X   Animal Freq 1    Ant    4 2    Cat    1 3    Dog    2 4    Ant    4 5    Dog    2 6    Ant    4 7    Ant    4 </code></pre>  <p>I suspect that the recommendation will be use apply in some way but I can't even get the function to work properly. I can get the following to work</p>  <pre><code>&gt; fn.freq &lt;- function(FreqTable, Variable){ +   return(subset(FreqTable, Animal == Variable)$freq) + } &gt; fn.freq(count(X),X[1,1]) [1]  </code></pre>  <p>But this still has <code>Animal</code> hard coded into the function when I really want it to be dynamic/a function variable but all my attempts at that fail miserably.</p>  <p>Any help greatly appreciated.</p>\n",
            "__label__c# <p>I am receiving this error while using a dllimport <code>Attempted to read or write protected memory. This is often an indication that other memory is corrupt</code> </p>  <pre><code>private const string dir2 = @\"C:\\NBioBSP.dll\";  [System.Runtime.InteropServices.DllImport(dir2, SetLastError = true, CallingConvention = System.Runtime.InteropServices.CallingConvention.Cdecl)] public static extern uint NBioAPI_FreeFIRHandle(IntPtr hHandle, IntPtr hFIR);  </code></pre>  <p>I call it like this</p>  <pre><code>uint resultado = NBioAPI_FreeFIRHandle(handle, handle); </code></pre>  <p>Anyone know what the issue can be</p>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j98z6ndSKO7",
        "colab_type": "code",
        "outputId": "e2384ec2-9f6b-4e33-8b03-e5ad8646f638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Train a model on this corpus\n",
        "import fasttext\n",
        "model = fasttext.train_supervised(input=py_base_dir+\"/sidecar/model_text.train\")\n",
        "model.save_model(py_base_dir+\"sidecar/model_text.bin\")\n",
        "model.test(py_base_dir+\"/sidecar/model_text.test\")\n",
        "# model.predict(\"NHQRegional SACO reviews ISP0421B form to ensure it meets the generic job profile matrix.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 0.424, 0.424)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxjXwbPQZWV6",
        "colab_type": "code",
        "outputId": "c2d1a270-0729-4aeb-e2e7-79db925c4aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import fasttext\n",
        "model = fasttext.train_supervised(input=py_base_dir+\"/sidecar/model_text.train\", epoch=25)\n",
        "model.save_model(py_base_dir+\"sidecar/model_text.bin\")\n",
        "model.test(py_base_dir+\"/sidecar/model_text.test\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 0.796, 0.796)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4wldp2HZmd8",
        "colab_type": "code",
        "outputId": "20b6df68-0428-4ed4-db91-3b94e9480c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import fasttext\n",
        "base_model = fasttext.train_supervised(input=py_base_dir+\"/sidecar/model_text.train\", \n",
        "                                       epoch=25, lr=1.0)\n",
        "base_model.save_model(py_base_dir+\"sidecar/model_text.bin\")\n",
        "base_model.test(py_base_dir+\"/sidecar/model_text.test\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 0.813, 0.813)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPIZN1obRbsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# note: -autotune-validation and many other options are available"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l76UcG_Po4z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "c93f1a13-c32e-462b-f26b-758032ef71e0"
      },
      "source": [
        "!pip3 install wget"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=7199ed9f5ebfabc9689b38f011c935788fe74db8a6916e702eaed9b9865a6dbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN2UBWhhZvMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import wget\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import spacy\n",
        "\n",
        "# load pretrained model\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "df.drop(columns=['[LocalizedFileNames]'],inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHPpXRYiTtLv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "outputId": "e595d6cf-dac6-426c-e373-1a402c20d83c"
      },
      "source": [
        "# for each row, concatenate the base model vector with the pretrained vector\n",
        "import numpy as np\n",
        "\n",
        "def makeCustomVec(row):\n",
        "  try:\n",
        "    txt = row[\"customV\"]\n",
        "    txt = ''.join(e for e in txt if e.isalnum() or e is ' ')\n",
        "    vec = base_model.get_sentence_vector(txt)\n",
        "    return [vec]\n",
        "  except:\n",
        "    return[np.zeros((1,96))]\n",
        "\n",
        "def makePretrainedVec(row):\n",
        "  try:\n",
        "    txt = row[\"body\"].replace(\"\\n\",\"\")\n",
        "    txt = ''.join(e for e in txt if e.isalnum() or e is ' ')\n",
        "    vec = nlp(txt).vector\n",
        "    return [vec]\n",
        "  except:\n",
        "    return[np.zeros((1,100))]\n",
        "\n",
        "display(Image(url= \"https://imgs.xkcd.com/comics/compiling.png\"))\n",
        "\n",
        "df[\"customV\"]=df[\"body\"].replace(\"\\n\",\"\")\n",
        "df[\"customV\"]=df.apply(makeCustomVec, axis=1)\n",
        "df[\"pretrainedV\"]=df.apply(makePretrainedVec, axis=1)\n",
        "display(df.head(10))\n",
        "for index,row in df.iterrows():\n",
        "  text = row['body'].replace(\"\\n\",\"\")\n",
        "  customV = row[\"customV\"]\n",
        "  pretrainedV = row[\"pretrainedV\"]\n",
        "  print(len(text),customV[0].shape,pretrainedV[0].shape)\n",
        "  break"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://imgs.xkcd.com/comics/compiling.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>tags</th>\n",
              "      <th>customV</th>\n",
              "      <th>pretrainedV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>&lt;p&gt;Having some problems with a javascript code...</td>\n",
              "      <td>php</td>\n",
              "      <td>[[-0.016572924, 0.005136265, 0.037449963, -0.0...</td>\n",
              "      <td>[[0.99867225, -1.1788186, -0.6309549, -1.04448...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>&lt;p&gt;I've got a large data set spanning many yea...</td>\n",
              "      <td>r</td>\n",
              "      <td>[[0.031693637, 0.08152228, -0.00843287, 0.0890...</td>\n",
              "      <td>[[5.5865903, 0.27739644, -2.6804729, 1.7886392...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958</th>\n",
              "      <td>&lt;p&gt;I have a dataset that I loaded into R using...</td>\n",
              "      <td>r</td>\n",
              "      <td>[[-0.004416636, 0.13198957, -0.04892641, 0.124...</td>\n",
              "      <td>[[0.65342474, -1.0588115, -1.1940643, -1.39399...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>&lt;p&gt;Interrupting the program below with Ctrl + ...</td>\n",
              "      <td>perl</td>\n",
              "      <td>[[-0.15919286, -0.1953033, -0.004372969, 0.240...</td>\n",
              "      <td>[[1.7554406, -1.3144658, -1.3427788, -1.416685...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>&lt;p&gt;I am trying to read data from MySQL and sho...</td>\n",
              "      <td>vb.net</td>\n",
              "      <td>[[0.14902955, -0.13814355, 0.04496123, -0.0713...</td>\n",
              "      <td>[[2.1630957, -1.4345362, -1.2440366, -1.542554...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>&lt;p&gt;I'm new to python, and I'm having problems ...</td>\n",
              "      <td>python</td>\n",
              "      <td>[[-0.01622935, 0.013932429, 0.016172351, 0.021...</td>\n",
              "      <td>[[1.78211, -1.3009847, -1.251538, -1.2445885, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>786</th>\n",
              "      <td>&lt;p&gt;This query takes about a minute to give res...</td>\n",
              "      <td>sql</td>\n",
              "      <td>[[0.059694316, -0.11732004, 0.055257183, -0.17...</td>\n",
              "      <td>[[0.71675146, -0.9493438, -0.5939291, -1.41842...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>&lt;p&gt;I'm getting list of strings from a method a...</td>\n",
              "      <td>c#</td>\n",
              "      <td>[[-0.005962717, 0.047765773, 0.032258432, -0.0...</td>\n",
              "      <td>[[0.9023432, -0.9161297, -1.0015508, -1.520370...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547</th>\n",
              "      <td>&lt;p&gt;So I know how to increment. I have the foll...</td>\n",
              "      <td>vb.net</td>\n",
              "      <td>[[0.14267495, -0.03386349, -0.025051177, 0.048...</td>\n",
              "      <td>[[1.0881171, -1.5441401, -0.6176418, -1.811053...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>&lt;pre&gt;&lt;code&gt;  import requests \\n        def pos...</td>\n",
              "      <td>python</td>\n",
              "      <td>[[-0.12212327, 0.11319813, 0.054229822, 0.0007...</td>\n",
              "      <td>[[1.602962, -1.6458162, 0.054127373, -1.437824...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  body  ...                                        pretrainedV\n",
              "372  <p>Having some problems with a javascript code...  ...  [[0.99867225, -1.1788186, -0.6309549, -1.04448...\n",
              "47   <p>I've got a large data set spanning many yea...  ...  [[5.5865903, 0.27739644, -2.6804729, 1.7886392...\n",
              "958  <p>I have a dataset that I loaded into R using...  ...  [[0.65342474, -1.0588115, -1.1940643, -1.39399...\n",
              "304  <p>Interrupting the program below with Ctrl + ...  ...  [[1.7554406, -1.3144658, -1.3427788, -1.416685...\n",
              "319  <p>I am trying to read data from MySQL and sho...  ...  [[2.1630957, -1.4345362, -1.2440366, -1.542554...\n",
              "970  <p>I'm new to python, and I'm having problems ...  ...  [[1.78211, -1.3009847, -1.251538, -1.2445885, ...\n",
              "786  <p>This query takes about a minute to give res...  ...  [[0.71675146, -0.9493438, -0.5939291, -1.41842...\n",
              "993  <p>I'm getting list of strings from a method a...  ...  [[0.9023432, -0.9161297, -1.0015508, -1.520370...\n",
              "547  <p>So I know how to increment. I have the foll...  ...  [[1.0881171, -1.5441401, -0.6176418, -1.811053...\n",
              "756  <pre><code>  import requests \\n        def pos...  ...  [[1.602962, -1.6458162, 0.054127373, -1.437824...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1465 (100,) (96,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfREaMtgPdXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "55672d9d-03ac-4c23-9efb-2cb198c8a8d4"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# we know the custom model performance alone, \n",
        "# let's see if augmenting with the pretrained vector helps\n",
        "x = np.zeros((df.shape[0],196))\n",
        "y = np.zeros((df.shape[0],10))\n",
        "\n",
        "labels=list(df[\"tags\"].unique())[:10]\n",
        "encoder={}\n",
        "for l in labels:\n",
        "  encoder[l]=labels.index(l)\n",
        "print(encoder)\n",
        "\n",
        "newIndex=0\n",
        "for index,row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "  customV = row[\"customV\"]\n",
        "  pretrainedV = row[\"pretrainedV\"]\n",
        "  label=row[\"tags\"]\n",
        "  if label in encoder.keys():\n",
        "    combinedV = np.hstack((customV[0],pretrainedV[0]))\n",
        "    x[newIndex,:]=combinedV\n",
        "    ix=encoder[label]\n",
        "    y_val=np.zeros(10)\n",
        "    y_val[ix]=1\n",
        "    y[newIndex,:]=y_val\n",
        "    newIndex+=1\n",
        "  else:\n",
        "    print(\"Someone is having a bad day\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 396/10001 [00:00<00:02, 3957.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'php': 0, 'r': 1, 'perl': 2, 'vb.net': 3, 'python': 4, 'sql': 5, 'c#': 6, 'java': 7, 'c++': 8, 'javascript': 9}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 4518/10001 [00:01<00:01, 4528.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Someone is having a bad day\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10001/10001 [00:02<00:00, 4352.66it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhxyV6l9h4dS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "258a9278-58db-42df-d789-a3e0260dee96"
      },
      "source": [
        "#test/train split\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Convolution1D\n",
        "from keras.losses import binary_crossentropy,sparse_categorical_crossentropy, mean_squared_error\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.20, random_state=42)\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8000, 196), (2001, 196), (8000, 10), (2001, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJLkEqDxl-Fv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a1891d0-ec43-4be0-e083-caff019eb1b9"
      },
      "source": [
        "from keras import regularizers\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim=196))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=256, validation_data=(x_test,y_test))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2001 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 1s 135us/step - loss: 1.8242 - acc: 0.3886 - val_loss: 1.4169 - val_acc: 0.5757\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 1.2098 - acc: 0.6359 - val_loss: 1.0098 - val_acc: 0.7056\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 0s 27us/step - loss: 0.9390 - acc: 0.7205 - val_loss: 0.8621 - val_acc: 0.7206\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.7861 - acc: 0.7616 - val_loss: 0.7632 - val_acc: 0.7651\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 0s 23us/step - loss: 0.7023 - acc: 0.7871 - val_loss: 0.7119 - val_acc: 0.7781\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 0s 27us/step - loss: 0.6532 - acc: 0.7999 - val_loss: 0.6589 - val_acc: 0.7956\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.6113 - acc: 0.8111 - val_loss: 0.6799 - val_acc: 0.7771\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 0s 24us/step - loss: 0.5959 - acc: 0.8141 - val_loss: 0.6018 - val_acc: 0.8191\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 0s 28us/step - loss: 0.5835 - acc: 0.8240 - val_loss: 0.6902 - val_acc: 0.7831\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.5644 - acc: 0.8266 - val_loss: 0.6076 - val_acc: 0.8106\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 0s 29us/step - loss: 0.5479 - acc: 0.8284 - val_loss: 0.6477 - val_acc: 0.7956\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.5407 - acc: 0.8337 - val_loss: 0.5975 - val_acc: 0.8151\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.5292 - acc: 0.8356 - val_loss: 0.6100 - val_acc: 0.8106\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 0s 29us/step - loss: 0.5278 - acc: 0.8385 - val_loss: 0.6183 - val_acc: 0.8056\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.5056 - acc: 0.8416 - val_loss: 0.6010 - val_acc: 0.8086\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 0s 28us/step - loss: 0.5117 - acc: 0.8431 - val_loss: 0.5994 - val_acc: 0.8176\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.4986 - acc: 0.8429 - val_loss: 0.5672 - val_acc: 0.8276\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 0s 24us/step - loss: 0.4884 - acc: 0.8465 - val_loss: 0.5685 - val_acc: 0.8321\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 0s 27us/step - loss: 0.4807 - acc: 0.8484 - val_loss: 0.5811 - val_acc: 0.8261\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.4762 - acc: 0.8551 - val_loss: 0.5979 - val_acc: 0.8146\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 0s 27us/step - loss: 0.4746 - acc: 0.8528 - val_loss: 0.6066 - val_acc: 0.8111\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 0s 27us/step - loss: 0.4663 - acc: 0.8535 - val_loss: 0.6370 - val_acc: 0.7971\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.4679 - acc: 0.8521 - val_loss: 0.5745 - val_acc: 0.8251\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.4617 - acc: 0.8563 - val_loss: 0.6388 - val_acc: 0.8051\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 0s 22us/step - loss: 0.4583 - acc: 0.8546 - val_loss: 0.5672 - val_acc: 0.8266\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 0s 23us/step - loss: 0.4512 - acc: 0.8568 - val_loss: 0.5846 - val_acc: 0.8271\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.4508 - acc: 0.8614 - val_loss: 0.5833 - val_acc: 0.8211\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 0s 24us/step - loss: 0.4495 - acc: 0.8598 - val_loss: 0.5644 - val_acc: 0.8301\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 0s 28us/step - loss: 0.4356 - acc: 0.8628 - val_loss: 0.5934 - val_acc: 0.8181\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.4347 - acc: 0.8618 - val_loss: 0.6165 - val_acc: 0.8161\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.4317 - acc: 0.8686 - val_loss: 0.5919 - val_acc: 0.8146\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 0s 24us/step - loss: 0.4298 - acc: 0.8635 - val_loss: 0.5663 - val_acc: 0.8301\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.4278 - acc: 0.8635 - val_loss: 0.5804 - val_acc: 0.8266\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 0s 23us/step - loss: 0.4257 - acc: 0.8662 - val_loss: 0.5898 - val_acc: 0.8256\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.4152 - acc: 0.8676 - val_loss: 0.5844 - val_acc: 0.8231\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.4163 - acc: 0.8662 - val_loss: 0.5801 - val_acc: 0.8236\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.4120 - acc: 0.8662 - val_loss: 0.5762 - val_acc: 0.8276\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.4054 - acc: 0.8698 - val_loss: 0.5928 - val_acc: 0.8261\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 0s 28us/step - loss: 0.4037 - acc: 0.8708 - val_loss: 0.5780 - val_acc: 0.8331\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 0s 25us/step - loss: 0.4036 - acc: 0.8731 - val_loss: 0.5746 - val_acc: 0.8316\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.3969 - acc: 0.8743 - val_loss: 0.6050 - val_acc: 0.8236\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.3921 - acc: 0.8759 - val_loss: 0.5893 - val_acc: 0.8276\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.3909 - acc: 0.8779 - val_loss: 0.6495 - val_acc: 0.8051\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 0s 27us/step - loss: 0.3798 - acc: 0.8775 - val_loss: 0.6059 - val_acc: 0.8171\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.3853 - acc: 0.8759 - val_loss: 0.5883 - val_acc: 0.8251\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 0s 24us/step - loss: 0.3805 - acc: 0.8776 - val_loss: 0.5893 - val_acc: 0.8251\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.3802 - acc: 0.8786 - val_loss: 0.5886 - val_acc: 0.8221\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 0s 23us/step - loss: 0.3712 - acc: 0.8787 - val_loss: 0.5839 - val_acc: 0.8246\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.3697 - acc: 0.8817 - val_loss: 0.5878 - val_acc: 0.8286\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 0s 24us/step - loss: 0.3662 - acc: 0.8836 - val_loss: 0.5919 - val_acc: 0.8266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3f9a953160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4k4XpTunnxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "25a3d5d2-e18b-41c1-87cb-e149351951c6"
      },
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2001/2001 [==============================] - 0s 74us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5918504187519821, 0.8265867067062516]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOepeBN6OJCF",
        "colab_type": "text"
      },
      "source": [
        "So we got a tiny (~1%) improvement for a lot of work...\n",
        "\n",
        "More text cleanup might help.\n",
        "\n",
        "Classifying sequences instead of average vectors might do better...\n",
        "\n",
        "Now let's go to the unsupervised side of the fence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai61qIxROdNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# More to explore:\n",
        "## https://github.com/huggingface/transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MdN7hVHaIIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}